{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "bxz9jMN4JqQi",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# NLP 2024\n",
        "# Lab 1: Classification models: Detecting hate speech\n",
        "\n",
        "The rise of social media has allowed the expression of opinions\n",
        "about debatable topics. X (ex Twitter) has emerged as a strong channel of communication to gather and disseminate news, to forecast outcome of elections and to exchange political events and discussions. It has also become an  important analytical tool for crime forecasting, tracking terrorist activities and detecting hate speech.\n",
        "\n",
        "Hate speech is commonly defined as \"any message that mocks\n",
        "or discriminates against a person or group based on specific\n",
        "characteristics such as color, ethnicity, gender, sexual orientation, nationality, religion, or other characteristics\". The\n",
        "amount of hate speech is steadily increasing due to X's\n",
        "popularity and the resulting big data from user-generated content.\n",
        "\n",
        "While NLP presents a promising approach to measure, detect and fight hate speech, that sounds more easy than it is. First of all, think about all the challenges that come with language evolution and language use in practice (e.g. sarcasm, slang, cultural variations) in understanding the true intent behind text. Moreover, there is an active discussion on what exactly consitutes hate speech from a legal perspective. Read more on this [here](https://aclanthology.org/2022.nllp-1.5.pdf).\n",
        "\n",
        "In this lab, we will explore the linguistic, technical and ethical spects of treating hate speech identification as a NLP classification task.\n",
        "\n",
        "By the end of this lab you should be able to:\n",
        "- Implement and/or use built-in functions to preprocess your data\n",
        "- Implement simple classification pipelines\n",
        "- Import and use `huggingface datasets` library\n",
        "- Import and use functions from `sklearn`\n",
        "- Evaluate classification results\n",
        "- Assess the difficulty of specific NLP tasks and propose solutions\n",
        "- Reflect on the ethical dimensions that a NLP model can have\n",
        "\n",
        "### Score breakdown\n",
        "\n",
        "Exercise | Points\n",
        "--- | ---\n",
        "[Exercise 1](#e1) | 7\n",
        "[Exercise 2](#e2) | 2\n",
        "[Exercise 3](#e3) | 2\n",
        "[Exercise 4](#e4) | 2\n",
        "[Exercise 5](#e5) | 5\n",
        "[Exercise 6](#e6) | 4\n",
        "[Exercise 7](#e7) | 5\n",
        "[Exercise 8](#e8) | 5\n",
        "[Exercise 9](#e9) | 5\n",
        "[Exercise 10](#e10) | 10\n",
        "[Exercise 11](#e11) | 3\n",
        "[Exercise 12](#e12) | 10\n",
        "[Exercise 13](#e13) | 5\n",
        "[Exercise 14](#e14) | 10\n",
        "[Exercise 15](#e15) | 10\n",
        "[Exercise 16](#e16) | 10\n",
        "[Exercise 17](#e17) | 5\n",
        "Total | 100\n",
        "\n",
        "This score will be scaled down to 1 and that will be your final lab score.\n",
        "\n",
        "### Instructions for delivery (Deadline: 6/May late night, wildcards possible)\n",
        "\n",
        "+ Make sure that you include a proper amount/mix of comments, results and code.\n",
        "+ In the end, make sure that all cells are executed properly and everything you need to show is in your (execucted) notebook.\n",
        "+ You are asked to deliver only your executed notebook file, .ipnyb and nothing else. Enjoy!\n",
        "+ Honor code applies to these tasks. Only individual work should be submitted.\n",
        "+ While you may talk with others about this lab, we ask that you write your solutions individually. If you do discuss specific tasks with others please include their names below.\n",
        "+ It is mandatory to list and disclose any website (or other resource) you used (e.g. stackoverflow) as well as any genAI tools (e.g. chatGPT) used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USvThR914dyx"
      },
      "source": [
        "Collaborators: list collaborators here\n",
        "\n",
        "**I talked with Jerry about...**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EuzalFF4dgg"
      },
      "source": [
        "Use of genAI tools (e.g. chatGPT), websites (e.g. stackoverflow): list websites where you found code (or other info) as well as include information on how you used genAI tools (e.g. prompts):\n",
        "\n",
        "I asked chatGPT about..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "0cN4xHB1JqQj",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Outline\n",
        "- [0 Setup](#0)\n",
        "- [1: Load Dataset](#1)\n",
        "- [2: Preprocess Dataset](#2)\n",
        "  - [Exercise 1](#e1)\n",
        "- [3: Build Vocabulary](#3)\n",
        "  - [Exercise 2](#e2)\n",
        "  - [Exercise 3](#e3)\n",
        "  - [Exercise 4](#e4)\n",
        "  - [Exercise 5](#e5)\n",
        "  - [Exercise 6](#e6)\n",
        "- [4: Build Handcrafted Classifier](43)\n",
        "  - [Exercise 7](#e7)\n",
        "  - [Exercise 8](#e8)\n",
        "- [5: Build Bag-of-Words](#5)\n",
        "  - [Exercise 9](#e9)\n",
        "- [6: Bag-of-Words with Naive Bayes](#6)\n",
        "  - [Exercise 10](#e10)\n",
        "- [7: Bag-of-Words with Logistic Regression](#7)\n",
        "  - [Exercise 11](#e11)\n",
        "  - [Exercise 12](#e12)\n",
        "- [8: TF-IDF](#8)\n",
        "  - [Exercise 13](#e13)\n",
        "  - [Exercise 14](#e14)\n",
        "- [9: Adding Handcrafted Features](#9)\n",
        "  - [Exercise 15](#e15)\n",
        "  - [Exercise 16](#e16)\n",
        "- [10: Reflection, Bias, Fairness, Ethics](#10)\n",
        "  - [Exercise 17](#e17)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "GUl1lKN2JqQj",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<a name='0'></a>\n",
        "## 0. Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "lbkqBA_9JqQj",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "We will be using huggingface datasets library ([https://huggingface.co/datasets](https://huggingface.co/datasets)). You can find the detailed documentation and tutorials here: [https://huggingface.co/docs/datasets/en/index](https://huggingface.co/docs/datasets/en/index)\n",
        "\n",
        "If you don't have it installed you can run the code below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-LrHJyhBJqQj",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets~=2.18.0 in /Users/wiksrivastava/anaconda3/lib/python3.11/site-packages (2.18.0)\n",
            "Requirement already satisfied: filelock in /Users/wiksrivastava/anaconda3/lib/python3.11/site-packages (from datasets~=2.18.0) (3.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/wiksrivastava/anaconda3/lib/python3.11/site-packages (from datasets~=2.18.0) (1.24.4)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /Users/wiksrivastava/anaconda3/lib/python3.11/site-packages (from datasets~=2.18.0) (16.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /Users/wiksrivastava/anaconda3/lib/python3.11/site-packages (from datasets~=2.18.0) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/wiksrivastava/anaconda3/lib/python3.11/site-packages (from datasets~=2.18.0) (0.3.6)\n",
            "Requirement already satisfied: pandas in /Users/wiksrivastava/anaconda3/lib/python3.11/site-packages (from datasets~=2.18.0) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /Users/wiksrivastava/anaconda3/lib/python3.11/site-packages (from datasets~=2.18.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /Users/wiksrivastava/anaconda3/lib/python3.11/site-packages (from datasets~=2.18.0) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /Users/wiksrivastava/anaconda3/lib/python3.11/site-packages (from datasets~=2.18.0) (2.0.2)\n",
            "Requirement already satisfied: multiprocess in /Users/wiksrivastava/anaconda3/lib/python3.11/site-packages (from datasets~=2.18.0) (0.70.14)\n",
            "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /Users/wiksrivastava/anaconda3/lib/python3.11/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets~=2.18.0) (2024.2.0)\n",
            "Requirement already satisfied: aiohttp in /Users/wiksrivastava/anaconda3/lib/python3.11/site-packages (from datasets~=2.18.0) (3.8.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /Users/wiksrivastava/anaconda3/lib/python3.11/site-packages (from datasets~=2.18.0) (0.23.0)\n",
            "Requirement already satisfied: packaging in /Users/wiksrivastava/anaconda3/lib/python3.11/site-packages (from datasets~=2.18.0) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/wiksrivastava/anaconda3/lib/python3.11/site-packages (from datasets~=2.18.0) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/wiksrivastava/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets~=2.18.0) (22.1.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /Users/wiksrivastava/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets~=2.18.0) (2.0.4)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/wiksrivastava/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets~=2.18.0) (6.0.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/wiksrivastava/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets~=2.18.0) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/wiksrivastava/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets~=2.18.0) (1.8.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/wiksrivastava/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets~=2.18.0) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/wiksrivastava/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets~=2.18.0) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/wiksrivastava/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.19.4->datasets~=2.18.0) (4.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/wiksrivastava/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets~=2.18.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/wiksrivastava/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets~=2.18.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/wiksrivastava/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets~=2.18.0) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/wiksrivastava/anaconda3/lib/python3.11/site-packages (from pandas->datasets~=2.18.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/wiksrivastava/anaconda3/lib/python3.11/site-packages (from pandas->datasets~=2.18.0) (2022.7)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /Users/wiksrivastava/anaconda3/lib/python3.11/site-packages (from pandas->datasets~=2.18.0) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /Users/wiksrivastava/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets~=2.18.0) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install -U datasets~=2.18.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "n-J0bd6hJqQj",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "We will also need those packages, so let's import them now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9E-maoXPJqQk",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import re    # for regular expressions\n",
        "from string import punctuation\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "dNyZiYIEJqQk",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<a name='1'></a>\n",
        "## 1. Load Dataset\n",
        "\n",
        "We will be using \"tweet_eval\" dataset and specifically the hate speech subset. You can find more info on how this dataset has been created in the following papers [[1](https://aclanthology.org/S19-2007.pdf)],[[2](https://aclanthology.org/2020.findings-emnlp.148.pdf)]. The dataset contains tweets from 2018 and the goal is to identify whether a tweet is hateful or not against any of the two target communities: immigrants and women. Make sure to read and reflect on how annotation was performed, since this can give you some insights on the modeling.\n",
        "\n",
        "In order to load a dataset simply call ```load_dataset``` function specifying the dataset name. You can find many more datasets at the huggingface website."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ljTlpzURJqQk",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 9000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 2970\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 1000\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "tweet_ds = datasets.load_dataset('tweet_eval', 'hate')\n",
        "print(tweet_ds)\n",
        "\n",
        "# this will make the dataset return the values as numpy arrays\n",
        "tweet_ds = tweet_ds.with_format(\"np\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "-M-GMrZqJqQm",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "The loaded dataset contains three subsets (\"train\", \"validation\", and \"test\"). Each consists of two columns: \"text\" and \"label\". Label of 0 means \"non-hate\" and label of 1 means \"hate\" We can access them like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NbCnVRuNJqQm",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'text': '@user nice new signage. Are you not concerned by Beatlemania -style hysterical crowds crongregating on youâ€¦', 'label': 0}\n",
            "{'text': 'A woman who you fucked multiple times saying yo dick small is a compliment you know u hit that spot ðŸ˜Ž', 'label': 1}\n",
            "{'text': '@user @user real talk do you have eyes or were they gouged out by a rapefugee?', 'label': 1}\n",
            "{'text': 'your girlfriend lookin at me like a groupie in this bitch!', 'label': 1}\n",
            "{'text': 'Hysterical woman like @user', 'label': 0}\n",
            "{'text': 'Me flirting- So tell me about your father...', 'label': 0}\n",
            "{'text': 'The Philippine Catholic bishops\\' work for migrant workers should focus on families who are \"paying the great...', 'label': 0}\n",
            "{'text': \"I AM NOT GOING AFTER YOUR EX BF YOU LIEING SACK OF SHIT ! I'm done with you dude that's why I dumped your ass cause your a lieing ðŸ˜‚ðŸ˜¡ bitch\", 'label': 1}\n"
          ]
        }
      ],
      "source": [
        "for i in range(8):\n",
        "    print(tweet_ds['train'][i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "2BH8vvzqJqQm",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<font color='red'>\n",
        "\n",
        "as part of the dataset inspection, it might be helpful to help them extract it and actually see all data points\n",
        "\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "W9ijz8qNJqQm",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "You can easily cast the dataset to the pandas DataFrame. We will do that below to plot the balance of the labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EwcVYkb7JqQn",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Axes: xlabel='label'>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAGrCAYAAADeuK1yAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8WgzjOAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAkYElEQVR4nO3df1DUdeLH8Rc/3FXEXVJhV0dIbrxSSi2pZO/KsszNI68umn6ZkT/qbLACJvWc8ay0+eLZD9LKvH4Y3p1O6t31Q5k0wsTKNZUGU0vt/HF4QwuawSangLDfPxo+IydaoLC85fmY2Rn3837vh/eHuY3nffazu2HBYDAoAAAAg4SHegEAAAAtRcAAAADjEDAAAMA4BAwAADAOAQMAAIxDwAAAAOMQMAAAwDiRoV5AW2loaFBZWZl69OihsLCwUC8HAAD8DMFgUD/88IP69u2r8PAzn2e5YAOmrKxM8fHxoV4GAABohUOHDqlfv35nHL9gA6ZHjx6SfvwFOByOEK8GAAD8HIFAQPHx8dbf8TO5YAOm8WUjh8NBwAAAYJifuvyDi3gBAIBxCBgAAGAcAgYAABjngr0GBgCA9lRfX6+6urpQL6PD69KliyIiIs55PwQMAADnIBgMyu/3q7KyMtRLMUZMTIzcbvc5fU4bAQMAwDlojJe4uDhFRUXx4alnEQwG9d///lcVFRWSpD59+rR6XwQMAACtVF9fb8VLr169Qr0cI3Tr1k2SVFFRobi4uFa/nMRFvAAAtFLjNS9RUVEhXolZGn9f53LNEAEDAMA54mWjljkfvy8CBgAAGIeAAQAAxuEiXgAA2kD/P+S36887OC+1XX9eqHEGBgCATuiGG25QZmZmh9/nmRAwAADAOAQMAACdzIMPPqiioiItWLBAYWFhCgsL08GDB7Vz506NGTNG0dHRcrlcGj9+vI4cOSJJ2rBhg2w2mz755BNrP/Pnz1dcXJzKy8vPuM+2wjUwF6D2ft0VodXZXvcGcO4WLFigvXv36vLLL9ecOXMk/fgdRddcc40mT56s3NxcHT9+XDNmzNBdd92l9evXWy8PjR8/Xtu3b9f+/fv1xz/+UatWrZLL5Wp2n7GxsW12DAQMAACdjNPplM1mU1RUlNxutyTpmWee0ZVXXqn/+7//s+YtWbJE8fHx2rt3ry655BI988wzKigo0MMPP6ydO3cqPT1dv/3tb8+4z7ZEwAAAAG3fvl0ff/yxoqOjTxvbt2+fLrnkEtlsNi1btkxDhgzRxRdfrNzc3BCs9EcEDAAA0LFjxzR27Fj96U9/Om3s1C9d3LRpkyTp6NGjOnr0qLp3795uazwVAQMAQCdks9lUX19v3R82bJj+8Y9/qH///oqMbD4P9u3bp6ysLL3++utasWKF0tPT9dFHHyk8PLzZfbYl3oUEAEAn1L9/f33++ec6ePCgjhw5ooyMDB09elT33nuvtm7dqn379mndunWaMGGC6uvrVV9fr/vvv19er1cTJkzQW2+9pS+//FLPP//8GffZ0NDQZuvnDAwAAG2go79D8IknnlB6erqSkpJ0/PhxHThwQJ999plmzJih0aNHq6amRhdffLFuueUWhYeHa+7cufr3v/+tNWvWSPrxZaXXXntN9957r0aPHq2hQ4c2u8/+/fu3yfrDgsFgsE32HGKBQEBOp1NVVVVyOByhXk674m3UnUtH/48kcCE7ceKEDhw4oMTERHXt2jXUyzHG2X5vP/fvNy8hAQAA4xAwAADAOAQMAAAwTosC5qmnnrK+36DxNnDgQGv8xIkTysjIUK9evRQdHa20tDSVl5c32UdpaalSU1MVFRWluLg4TZs2TSdPnmwyZ8OGDRo2bJjsdrsGDBigvLy81h8hAAC44LT4DMxll12mb7/91rp9+umn1lhWVpZWr16tVatWqaioSGVlZbrjjjus8fr6eqWmpqq2tlabNm3S0qVLlZeXp9mzZ1tzDhw4oNTUVI0cOVIlJSXKzMzU5MmTtW7dunM8VAAA2kZbvl34QnQ+fl8tfht1ZGRks99xUFVVpTfffFPLly/XjTfeKEl66623NGjQIG3evFkpKSn68MMP9dVXX+mjjz6Sy+XSFVdcoblz52rGjBl66qmnZLPZtHjxYiUmJlrvKx80aJA+/fRT5ebmyuv1nnFdNTU1qqmpse4HAoGWHhoAAC1is9kUHh6usrIyxcbGymazKSwsLNTL6rCCwaBqa2t1+PBhhYeHy2aztXpfLQ6Yb775Rn379lXXrl3l8XiUk5OjhIQEFRcXq66uTqNGjbLmDhw4UAkJCfL5fEpJSZHP59PgwYPlcrmsOV6vV4888oh27dqlK6+8Uj6fr8k+GudkZmaedV05OTl6+umnW3o4AAC0Wnh4uBITE/Xtt9+qrKws1MsxRlRUlBISEqxP8G2NFgXM8OHDlZeXp0svvVTffvutnn76aV133XXauXOn/H6/bDabYmJimjzG5XLJ7/dLkvx+f5N4aRxvHDvbnEAgoOPHj6tbt27Nrm3mzJnKzs627gcCAcXHx7fk8AAAaDGbzaaEhASdPHmy3T5G32QRERGKjIw85zNVLQqYMWPGWP8eMmSIhg8frosvvlgrV648Y1i0F7vdLrvdHtI1AAA6p7CwMHXp0kVdunQJ9VI6jXN6G3VMTIwuueQS/etf/5Lb7VZtba0qKyubzCkvL7eumXG73ae9K6nx/k/NcTgcIY8kAADQMZxTwBw7dkz79u1Tnz59lJycrC5duqiwsNAa37Nnj0pLS+XxeCRJHo9HO3bsUEVFhTWnoKBADodDSUlJ1pxT99E4p3EfAAAALQqYJ554QkVFRTp48KA2bdqk3/3ud4qIiNC9994rp9OpSZMmKTs7Wx9//LGKi4s1YcIEeTwepaSkSJJGjx6tpKQkjR8/Xtu3b9e6des0a9YsZWRkWC//TJkyRfv379f06dO1e/duLVq0SCtXrlRWVtb5P3oAAGCkFl0D85///Ef33nuvvvvuO8XGxuraa6/V5s2bFRsbK0nKzc1VeHi40tLSVFNTI6/Xq0WLFlmPj4iI0Jo1a/TII4/I4/Goe/fuSk9P15w5c6w5iYmJys/PV1ZWlhYsWKB+/frpjTfeOOtbqAEAQOfCt1FfgPg26s6Fb6MGcCHh26gBAMAFi4ABAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHEIGAAAYBwCBgAAGIeAAQAAxiFgAACAcQgYAABgHAIGAAAYh4ABAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHEIGAAAYBwCBgAAGIeAAQAAxiFgAACAcQgYAABgHAIGAAAYh4ABAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHEIGAAAYBwCBgAAGIeAAQAAxiFgAACAcQgYAABgHAIGAAAYh4ABAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHEIGAAAYBwCBgAAGIeAAQAAxiFgAACAcQgYAABgHAIGAAAYh4ABAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHEIGAAAYJxzCph58+YpLCxMmZmZ1rYTJ04oIyNDvXr1UnR0tNLS0lReXt7kcaWlpUpNTVVUVJTi4uI0bdo0nTx5ssmcDRs2aNiwYbLb7RowYIDy8vLOZakAAOAC0uqA2bp1q/785z9ryJAhTbZnZWVp9erVWrVqlYqKilRWVqY77rjDGq+vr1dqaqpqa2u1adMmLV26VHl5eZo9e7Y158CBA0pNTdXIkSNVUlKizMxMTZ48WevWrWvtcgEAwAWkVQFz7NgxjRs3Tq+//rouuugia3tVVZXefPNNvfDCC7rxxhuVnJyst956S5s2bdLmzZslSR9++KG++uor/e1vf9MVV1yhMWPGaO7cuXrllVdUW1srSVq8eLESExP1/PPPa9CgQZo6daruvPNO5ebmnodDBgAApmtVwGRkZCg1NVWjRo1qsr24uFh1dXVNtg8cOFAJCQny+XySJJ/Pp8GDB8vlcllzvF6vAoGAdu3aZc353317vV5rH82pqalRIBBocgMAABemyJY+4O2339YXX3yhrVu3njbm9/tls9kUExPTZLvL5ZLf77fmnBovjeONY2ebEwgEdPz4cXXr1u20n52Tk6Onn366pYcDAAAM1KIzMIcOHdLjjz+uZcuWqWvXrm21plaZOXOmqqqqrNuhQ4dCvSQAANBGWhQwxcXFqqio0LBhwxQZGanIyEgVFRVp4cKFioyMlMvlUm1trSorK5s8rry8XG63W5LkdrtPe1dS4/2fmuNwOJo9+yJJdrtdDoejyQ0AAFyYWhQwN910k3bs2KGSkhLrdtVVV2ncuHHWv7t06aLCwkLrMXv27FFpaak8Ho8kyePxaMeOHaqoqLDmFBQUyOFwKCkpyZpz6j4a5zTuAwAAdG4tugamR48euvzyy5ts6969u3r16mVtnzRpkrKzs9WzZ085HA49+uij8ng8SklJkSSNHj1aSUlJGj9+vObPny+/369Zs2YpIyNDdrtdkjRlyhS9/PLLmj59uiZOnKj169dr5cqVys/PPx/HDAAADNfii3h/Sm5ursLDw5WWlqaamhp5vV4tWrTIGo+IiNCaNWv0yCOPyOPxqHv37kpPT9ecOXOsOYmJicrPz1dWVpYWLFigfv366Y033pDX6z3fywUAAAYKCwaDwVAvoi0EAgE5nU5VVVV1uuth+v+BM1WdycF5qaFeAgCcNz/37zffhQQAAIxDwAAAAOMQMAAAwDgEDAAAMA4BAwAAjEPAAAAA4xAwAADAOAQMAAAwznn/JF4AQNvhgyo7Fz6o8sw4AwMAAIxDwAAAAOMQMAAAwDgEDAAAMA4BAwAAjEPAAAAA4xAwAADAOAQMAAAwDgEDAACMQ8AAAADjEDAAAMA4BAwAADAOAQMAAIxDwAAAAOMQMAAAwDgEDAAAMA4BAwAAjEPAAAAA4xAwAADAOAQMAAAwDgEDAACMQ8AAAADjEDAAAMA4BAwAADAOAQMAAIxDwAAAAOMQMAAAwDgEDAAAMA4BAwAAjEPAAAAA4xAwAADAOAQMAAAwDgEDAACMQ8AAAADjEDAAAMA4BAwAADAOAQMAAIxDwAAAAOMQMAAAwDgEDAAAMA4BAwAAjEPAAAAA4xAwAADAOAQMAAAwDgEDAACMQ8AAAADjEDAAAMA4BAwAADBOiwLm1Vdf1ZAhQ+RwOORwOOTxePTBBx9Y4ydOnFBGRoZ69eql6OhopaWlqby8vMk+SktLlZqaqqioKMXFxWnatGk6efJkkzkbNmzQsGHDZLfbNWDAAOXl5bX+CAEAwAWnRQHTr18/zZs3T8XFxdq2bZtuvPFG3Xbbbdq1a5ckKSsrS6tXr9aqVatUVFSksrIy3XHHHdbj6+vrlZqaqtraWm3atElLly5VXl6eZs+ebc05cOCAUlNTNXLkSJWUlCgzM1OTJ0/WunXrztMhAwAA04UFg8HgueygZ8+eevbZZ3XnnXcqNjZWy5cv15133ilJ2r17twYNGiSfz6eUlBR98MEHuvXWW1VWViaXyyVJWrx4sWbMmKHDhw/LZrNpxowZys/P186dO62fcc8996iyslJr16494zpqampUU1Nj3Q8EAoqPj1dVVZUcDse5HKJx+v8hP9RLQDs6OC811EtAO+L53bl0xud3IBCQ0+n8yb/frb4Gpr6+Xm+//baqq6vl8XhUXFysuro6jRo1ypozcOBAJSQkyOfzSZJ8Pp8GDx5sxYskeb1eBQIB6yyOz+drso/GOY37OJOcnBw5nU7rFh8f39pDAwAAHVyLA2bHjh2Kjo6W3W7XlClT9M477ygpKUl+v182m00xMTFN5rtcLvn9fkmS3+9vEi+N441jZ5sTCAR0/PjxM65r5syZqqqqsm6HDh1q6aEBAABDRLb0AZdeeqlKSkpUVVWlv//970pPT1dRUVFbrK1F7Ha77HZ7qJcBAADaQYsDxmazacCAAZKk5ORkbd26VQsWLNDdd9+t2tpaVVZWNjkLU15eLrfbLUlyu93asmVLk/01vkvp1Dn/+86l8vJyORwOdevWraXLBQAAF6Bz/hyYhoYG1dTUKDk5WV26dFFhYaE1tmfPHpWWlsrj8UiSPB6PduzYoYqKCmtOQUGBHA6HkpKSrDmn7qNxTuM+AAAAWnQGZubMmRozZowSEhL0ww8/aPny5dqwYYPWrVsnp9OpSZMmKTs7Wz179pTD4dCjjz4qj8ejlJQUSdLo0aOVlJSk8ePHa/78+fL7/Zo1a5YyMjKsl3+mTJmil19+WdOnT9fEiRO1fv16rVy5Uvn5XHkPAAB+1KKAqaio0AMPPKBvv/1WTqdTQ4YM0bp163TzzTdLknJzcxUeHq60tDTV1NTI6/Vq0aJF1uMjIiK0Zs0aPfLII/J4POrevbvS09M1Z84ca05iYqLy8/OVlZWlBQsWqF+/fnrjjTfk9XrP0yEDAADTnfPnwHRUP/d95BciPieic+mMnxPRmfH87lw64/O7zT8HBgAAIFQIGAAAYBwCBgAAGIeAAQAAxiFgAACAcQgYAABgHAIGAAAYh4ABAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHEIGAAAYBwCBgAAGIeAAQAAxiFgAACAcQgYAABgHAIGAAAYh4ABAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHEIGAAAYBwCBgAAGIeAAQAAxiFgAACAcQgYAABgHAIGAAAYh4ABAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHEIGAAAYBwCBgAAGIeAAQAAxiFgAACAcQgYAABgHAIGAAAYh4ABAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHEIGAAAYBwCBgAAGIeAAQAAxiFgAACAcQgYAABgHAIGAAAYh4ABAADGaVHA5OTk6Oqrr1aPHj0UFxen22+/XXv27Gky58SJE8rIyFCvXr0UHR2ttLQ0lZeXN5lTWlqq1NRURUVFKS4uTtOmTdPJkyebzNmwYYOGDRsmu92uAQMGKC8vr3VHCAAALjgtCpiioiJlZGRo8+bNKigoUF1dnUaPHq3q6mprTlZWllavXq1Vq1apqKhIZWVluuOOO6zx+vp6paamqra2Vps2bdLSpUuVl5en2bNnW3MOHDig1NRUjRw5UiUlJcrMzNTkyZO1bt2683DIAADAdGHBYDDY2gcfPnxYcXFxKioq0ogRI1RVVaXY2FgtX75cd955pyRp9+7dGjRokHw+n1JSUvTBBx/o1ltvVVlZmVwulyRp8eLFmjFjhg4fPiybzaYZM2YoPz9fO3futH7WPffco8rKSq1du/ZnrS0QCMjpdKqqqkoOh6O1h2ik/n/ID/US0I4OzksN9RLQjnh+dy6d8fn9c/9+n9M1MFVVVZKknj17SpKKi4tVV1enUaNGWXMGDhyohIQE+Xw+SZLP59PgwYOteJEkr9erQCCgXbt2WXNO3UfjnMZ9NKempkaBQKDJDQAAXJhaHTANDQ3KzMzUr3/9a11++eWSJL/fL5vNppiYmCZzXS6X/H6/NefUeGkcbxw725xAIKDjx483u56cnBw5nU7rFh8f39pDAwAAHVyrAyYjI0M7d+7U22+/fT7X02ozZ85UVVWVdTt06FColwQAANpIZGseNHXqVK1Zs0YbN25Uv379rO1ut1u1tbWqrKxschamvLxcbrfbmrNly5Ym+2t8l9Kpc/73nUvl5eVyOBzq1q1bs2uy2+2y2+2tORwAAGCYFp2BCQaDmjp1qt555x2tX79eiYmJTcaTk5PVpUsXFRYWWtv27Nmj0tJSeTweSZLH49GOHTtUUVFhzSkoKJDD4VBSUpI159R9NM5p3AcAAOjcWnQGJiMjQ8uXL9d7772nHj16WNesOJ1OdevWTU6nU5MmTVJ2drZ69uwph8OhRx99VB6PRykpKZKk0aNHKykpSePHj9f8+fPl9/s1a9YsZWRkWGdQpkyZopdfflnTp0/XxIkTtX79eq1cuVL5+Vx9DwAAWngG5tVXX1VVVZVuuOEG9enTx7qtWLHCmpObm6tbb71VaWlpGjFihNxut/75z39a4xEREVqzZo0iIiLk8Xh0//3364EHHtCcOXOsOYmJicrPz1dBQYGGDh2q559/Xm+88Ya8Xu95OGQAAGC6c/ocmI6Mz4FBZ9EZPyeiM+P53bl0xud3u3wODAAAQCgQMAAAwDgEDAAAMA4BAwAAjEPAAAAA4xAwAADAOAQMAAAwDgEDAACMQ8AAAADjEDAAAMA4BAwAADAOAQMAAIxDwAAAAOMQMAAAwDgEDAAAMA4BAwAAjEPAAAAA4xAwAADAOAQMAAAwDgEDAACMQ8AAAADjEDAAAMA4BAwAADAOAQMAAIxDwAAAAOMQMAAAwDgEDAAAMA4BAwAAjEPAAAAA4xAwAADAOAQMAAAwDgEDAACMQ8AAAADjEDAAAMA4BAwAADAOAQMAAIxDwAAAAOMQMAAAwDgEDAAAMA4BAwAAjEPAAAAA4xAwAADAOAQMAAAwDgEDAACMQ8AAAADjEDAAAMA4BAwAADAOAQMAAIxDwAAAAOMQMAAAwDgEDAAAMA4BAwAAjEPAAAAA4xAwAADAOAQMAAAwDgEDAACMQ8AAAADjtDhgNm7cqLFjx6pv374KCwvTu+++22Q8GAxq9uzZ6tOnj7p166ZRo0bpm2++aTLn6NGjGjdunBwOh2JiYjRp0iQdO3asyZwvv/xS1113nbp27ar4+HjNnz+/5UcHAAAuSC0OmOrqag0dOlSvvPJKs+Pz58/XwoULtXjxYn3++efq3r27vF6vTpw4Yc0ZN26cdu3apYKCAq1Zs0YbN27Uww8/bI0HAgGNHj1aF198sYqLi/Xss8/qqaee0muvvdaKQwQAABeayJY+YMyYMRozZkyzY8FgUC+++KJmzZql2267TZL0l7/8RS6XS++++67uueceff3111q7dq22bt2qq666SpL00ksv6Te/+Y2ee+459e3bV8uWLVNtba2WLFkim82myy67TCUlJXrhhReahM6pampqVFNTY90PBAItPTQAAGCI83oNzIEDB+T3+zVq1Chrm9Pp1PDhw+Xz+SRJPp9PMTExVrxI0qhRoxQeHq7PP//cmjNixAjZbDZrjtfr1Z49e/T99983+7NzcnLkdDqtW3x8/Pk8NAAA0IGc14Dx+/2SJJfL1WS7y+Wyxvx+v+Li4pqMR0ZGqmfPnk3mNLePU3/G/5o5c6aqqqqs26FDh879gAAAQIfU4peQOiq73S673R7qZQAAgHZwXs/AuN1uSVJ5eXmT7eXl5daY2+1WRUVFk/GTJ0/q6NGjTeY0t49TfwYAAOi8zmvAJCYmyu12q7Cw0NoWCAT0+eefy+PxSJI8Ho8qKytVXFxszVm/fr0aGho0fPhwa87GjRtVV1dnzSkoKNCll16qiy666HwuGQAAGKjFAXPs2DGVlJSopKRE0o8X7paUlKi0tFRhYWHKzMzUM888o/fff187duzQAw88oL59++r222+XJA0aNEi33HKLHnroIW3ZskWfffaZpk6dqnvuuUd9+/aVJN13332y2WyaNGmSdu3apRUrVmjBggXKzs4+bwcOAADM1eJrYLZt26aRI0da9xujIj09XXl5eZo+fbqqq6v18MMPq7KyUtdee63Wrl2rrl27Wo9ZtmyZpk6dqptuuknh4eFKS0vTwoULrXGn06kPP/xQGRkZSk5OVu/evTV79uwzvoUaAAB0LmHBYDAY6kW0hUAgIKfTqaqqKjkcjlAvp131/0N+qJeAdnRwXmqol4B2xPO7c+mMz++f+/eb70ICAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHEIGAAAYBwCBgAAGIeAAQAAxiFgAACAcQgYAABgHAIGAAAYh4ABAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHEIGAAAYBwCBgAAGIeAAQAAxiFgAACAcQgYAABgHAIGAAAYh4ABAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHEIGAAAYBwCBgAAGIeAAQAAxiFgAACAcQgYAABgHAIGAAAYh4ABAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHEIGAAAYBwCBgAAGIeAAQAAxiFgAACAcQgYAABgHAIGAAAYh4ABAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHEIGAAAYBwCBgAAGKdDB8wrr7yi/v37q2vXrho+fLi2bNkS6iUBAIAOoMMGzIoVK5Sdna0nn3xSX3zxhYYOHSqv16uKiopQLw0AAIRYhw2YF154QQ899JAmTJigpKQkLV68WFFRUVqyZEmolwYAAEIsMtQLaE5tba2Ki4s1c+ZMa1t4eLhGjRoln8/X7GNqampUU1Nj3a+qqpIkBQKBtl1sB9RQ899QLwHtqDP+b7wz4/nduXTG53fjMQeDwbPO65ABc+TIEdXX18vlcjXZ7nK5tHv37mYfk5OTo6effvq07fHx8W2yRqCjcL4Y6hUAaCud+fn9ww8/yOl0nnG8QwZMa8ycOVPZ2dnW/YaGBh09elS9evVSWFhYCFeG9hAIBBQfH69Dhw7J4XCEejkAziOe351LMBjUDz/8oL59+551XocMmN69eysiIkLl5eVNtpeXl8vtdjf7GLvdLrvd3mRbTExMWy0RHZTD4eA/cMAFiud353G2My+NOuRFvDabTcnJySosLLS2NTQ0qLCwUB6PJ4QrAwAAHUGHPAMjSdnZ2UpPT9dVV12la665Ri+++KKqq6s1YcKEUC8NAACEWIcNmLvvvluHDx/W7Nmz5ff7dcUVV2jt2rWnXdgLSD++hPjkk0+e9jIiAPPx/EZzwoI/9T4lAACADqZDXgMDAABwNgQMAAAwDgEDAACMQ8AAAADjEDAAAMA4HfZt1MDZHDlyREuWLJHP55Pf75ckud1u/epXv9KDDz6o2NjYEK8QANCWOAMD42zdulWXXHKJFi5cKKfTqREjRmjEiBFyOp1auHChBg4cqG3btoV6mQDawKFDhzRx4sRQLwMdAJ8DA+OkpKRo6NChWrx48Wlf1BkMBjVlyhR9+eWX8vl8IVohgLayfft2DRs2TPX19aFeCkKMl5BgnO3btysvL6/ZbxkPCwtTVlaWrrzyyhCsDMC5ev/99886vn///nZaCTo6AgbGcbvd2rJliwYOHNjs+JYtW/jKCcBQt99+u8LCwnS2Fwea+z8v6HwIGBjniSee0MMPP6zi4mLddNNNVqyUl5ersLBQr7/+up577rkQrxJAa/Tp00eLFi3Sbbfd1ux4SUmJkpOT23lV6IgIGBgnIyNDvXv3Vm5urhYtWmS9Fh4REaHk5GTl5eXprrvuCvEqAbRGcnKyiouLzxgwP3V2Bp0HF/HCaHV1dTpy5IgkqXfv3urSpUuIVwTgXHzyySeqrq7WLbfc0ux4dXW1tm3bpuuvv76dV4aOhoABAADG4XNgAACAcQgYAABgHAIGAAAYh4ABAADGIWAAhMQNN9ygzMzMnzV3w4YNCgsLU2Vl5Tn9zP79++vFF188p30A6BgIGAAAYBwCBgAAGIeAARByf/3rX3XVVVepR48ecrvduu+++1RRUXHavM8++0xDhgxR165dlZKSop07dzYZ//TTT3XdddepW7duio+P12OPPabq6ur2OgwA7YiAARBydXV1mjt3rrZv3653331XBw8e1IMPPnjavGnTpun555/X1q1bFRsbq7Fjx6qurk6StG/fPt1yyy1KS0vTl19+qRUrVujTTz/V1KlT2/loALQHvgsJQMhNnDjR+vcvfvELLVy4UFdffbWOHTum6Ohoa+zJJ5/UzTffLElaunSp+vXrp3feeUd33XWXcnJyNG7cOOvC4F/+8pdauHChrr/+er366qvq2rVrux4TgLbFGRgAIVdcXKyxY8cqISFBPXr0sL7nprS0tMk8j8dj/btnz5669NJL9fXXX0uStm/frry8PEVHR1s3r9erhoYGHThwoP0OBkC74AwMgJCqrq6W1+uV1+vVsmXLFBsbq9LSUnm9XtXW1v7s/Rw7dky///3v9dhjj502lpCQcD6XDKADIGAAhNTu3bv13Xffad68eYqPj5ckbdu2rdm5mzdvtmLk+++/1969ezVo0CBJ0rBhw/TVV19pwIAB7bNwACHFS0gAQiohIUE2m00vvfSS9u/fr/fff19z585tdu6cOXNUWFionTt36sEHH1Tv3r11++23S5JmzJihTZs2aerUqSopKdE333yj9957j4t4gQsUAQMgpGJjY5WXl6dVq1YpKSlJ8+bN03PPPdfs3Hnz5unxxx9XcnKy/H6/Vq9eLZvNJkkaMmSIioqKtHfvXl133XW68sorNXv2bPXt27c9DwdAOwkLBoPBUC8CAACgJTgDAwAAjEPAAAAA4xAwAADAOAQMAAAwDgEDAACMQ8AAAADjEDAAAMA4BAwAADAOAQMAAIxDwAAAAOMQMAAAwDj/D8uOP1AP5ELTAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pd.DataFrame(tweet_ds['train']).groupby('label').count().plot.bar()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "h8NhtvljJqQn",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "We can filter the examples based on the label. We will use ```filter()``` method. See this link for more details [https://huggingface.co/docs/datasets/en/use_dataset](https://huggingface.co/docs/datasets/en/use_dataset)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RqX3YWMEJqQn",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'text': '@user nice new signage. Are you not concerned by Beatlemania -style hysterical crowds crongregating on youâ€¦', 'label': 0}\n",
            "{'text': 'Hysterical woman like @user', 'label': 0}\n",
            "{'text': 'Me flirting- So tell me about your father...', 'label': 0}\n",
            "{'text': 'The Philippine Catholic bishops\\' work for migrant workers should focus on families who are \"paying the great...', 'label': 0}\n",
            "{'text': 'When cuffin season is finally over', 'label': 0}\n",
            "{'text': \"Only that in you which is me can hear what I'm saying. ~Baba Ram Dass\", 'label': 0}\n",
            "{'text': 'More migrants take sea route to #Spain than Italy this year: UN', 'label': 0}\n",
            "{'text': 'Essential reading for those in Scribner, Nebraska who are considering an anti-immigrant ballot measure.', 'label': 0}\n"
          ]
        }
      ],
      "source": [
        "no_hate = tweet_ds['train'].filter(lambda e: e['label'] == 0)\n",
        "for i in range(8):\n",
        "    print(no_hate[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "piHOC8PXJqQn",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Let's use the ```filter()``` method to remove empty entries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xLcib2x5JqQn",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 8993\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 2970\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 999\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "tweet_ds = tweet_ds.filter(lambda e: e['text'] != '' and e['text'] is not None)\n",
        "print(tweet_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "uE-RHPN2JqQn",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<a name='2'></a>\n",
        "## 2. Preprocess Dataset\n",
        "\n",
        "In this section we will preprocess the dataset by cleaning and tokenizing the entries. You will use the knowledge and skills from the previous lab.\n",
        "\n",
        "Datasets library contains a very useful method ```map```. It expects a function that will receive an example from the dataset. This function will be applied to all entries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "WTuR0bMNJqQo",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<a name='e1'></a>\n",
        "### Exercise 1 (points 7)\n",
        "\n",
        "Fill in the function in order to clean the examples. We already provide some pre-processing (e.g. turn text to a lower case) and some regular expressions to handle some punctuation special cases, however the goal is for you to implement your own operations. You might want to inspect the dataset (e.g. check several data points) before you implement this method and also you might need to revisit it after you have seen the first classification results. For every operation, try to also include the intuition behind it (unless it's obvious).\n",
        "\n",
        "There is no exclusive list of pre-processing you need to have (since many decisions will depend on your observations). Try to include different examples of handling different language issues with specific regular expressions or other pre-processing decisions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "T2EmO2qmJqQo",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def clean(example):\n",
        "    \"\"\"\n",
        "    Cleans the example from the Dataset\n",
        "    Args:\n",
        "        example: an example from the Dataset\n",
        "\n",
        "    Returns: update example containing 'clean' column\n",
        "\n",
        "    \"\"\"\n",
        "    text = example['text']\n",
        "\n",
        "    # Empty text\n",
        "    if type(text) not in (str, np.str_) or text=='':\n",
        "        example['clean'] = ''\n",
        "        return example\n",
        "\n",
        "    # 'text' from the example can be of type numpy.str_, let's convert it to a python str\n",
        "    text = str(text)\n",
        "    text = text.lower()\n",
        "\n",
        "    # Clean the text\n",
        "    text = re.sub(\"\\'s\", \" \", text) # we have cases like \"Sam is\" or \"Sam's\" (i.e. his) these two cases aren't separable, I choose to compromise are kill \"'s\" directly\n",
        "    text = re.sub(\" whats \", \" what is \", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"\\'ve\", \" have \", text)\n",
        "\n",
        "    #you might need more\n",
        "    #add them here\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'@[^\\s]+', '', text)\n",
        "    \n",
        "    text = re.sub(r'#[^\\s]+', '', text)\n",
        "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)    \n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    text = re.sub(\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(\"\\'re\", \" are \", text)\n",
        "    text = re.sub(\"\\'d\", \" would \", text)\n",
        "    text = re.sub(\"\\'m\", \" am \", text)\n",
        "    text = re.sub(\"n\\'t\", \" not \", text)\n",
        "\n",
        "    text = re.sub(\" lol \", ' laugh out loud ', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\" brb \", ' be right back ', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\" btw \", ' by the way ', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\" omg \", ' oh my god ', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\" wtf \", ' what the fuck ', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\" idk \", ' I don\\'t know ', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\" imo \", ' in my opinion ', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\" rofl \", ' rolling on the floor laughing ', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\" ttyl \", ' talk to you later ', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\" gtg \", ' got to go ', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\" omw \", ' on my way ', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\" fwiw \", ' for what it\\'s worth ', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\" afaik \", ' as far as I know ', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\" irl \", ' in real life ', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\" np \", ' no problem ', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\" rt \", ' retweet ', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\" smh \", ' shaking my head ', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\" tbt \", ' throwback thursday ', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\" tyt \", ' take your time ', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\" icymi \", ' in case you missed it ', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\" yolo \", ' you only live once ', text, flags=re.IGNORECASE)\n",
        "    \n",
        "    # Tokenization\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ### YOUR CODE ENDS HERE\n",
        "\n",
        "    # remove comma between numbers, i.e. 15,000 -> 15000\n",
        "    text = re.sub('(?<=[0-9])\\,(?=[0-9])', \"\", text)\n",
        "\n",
        "    # Update the example with the cleaned text\n",
        "    example['clean'] = text.strip()\n",
        "    return example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "PLNEnuVoJqQo",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "This is an example of applying the ```clean()``` function you just wrote to a single entry of the dataset. The function added a 'clean' field to the example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ClLlgEPtJqQo",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'text': 'Cory Booker and Kamala Harris competing for Most Hysterical Woman at the Kavanaugh hearings, Coulter hilariously tweeted.And yes, liberals immediately got triggered on Twitter, saying her joke was offensive. To them we say, suck it up, snowflakes.', 'label': 1}\n",
            "{'text': 'Cory Booker and Kamala Harris competing for Most Hysterical Woman at the Kavanaugh hearings, Coulter hilariously tweeted.And yes, liberals immediately got triggered on Twitter, saying her joke was offensive. To them we say, suck it up, snowflakes.', 'label': 1, 'clean': 'cory booker and kamala harris competing for most hysterical woman at the kavanaugh hearings coulter hilariously tweetedand yes liberals immediately got triggered on twitter saying her joke was offensive to them we say suck it up snowflakes'}\n"
          ]
        }
      ],
      "source": [
        "print(tweet_ds['train'][10])\n",
        "print(clean(tweet_ds['train'][10]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "5gCMr7svJqQo",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Let's finally use the ```map()``` method and apply your `clean()` function to all entries of the dataset. You can see that the ```clean``` column has been added to each split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jSD3kfxUJqQo",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label', 'clean'],\n",
            "        num_rows: 8993\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label', 'clean'],\n",
            "        num_rows: 2970\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text', 'label', 'clean'],\n",
            "        num_rows: 999\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "tweet_ds = tweet_ds.map(clean)\n",
        "print(tweet_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "xnPHE9KVJqQo",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<a name='3'></a>\n",
        "## 3. Build Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Q2mkeRlEJqQp",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Feature engineering is when NLP task-specific knowledge comes in handy, and make it more likely for a simple classifier to learn the task. This requires us to index tokens and create meaningful representations out of them.\n",
        "\n",
        "Firstm we have to create a vocabulary. In the previous section, we implemeneted the tokenization and the cleaning.\n",
        "\n",
        "In this section we will start by building the vocabulary for your dataset (aka the number of different words in the corpus). We will build it based on the cleaned text of the `train` split. We will investigate some properties of corpora (e.g. Zipf's law, most common words for hate speech language).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "YenDNVQSJqQp",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<a name='e2'></a>\n",
        "### Exercise 2 (points 2)\n",
        "\n",
        "Fill in the following function so that the ```Counter``` is returned containing all words of a sentence with the number of occurrences. Check the documentation if needed [https://docs.python.org/3/library/collections.html#collections.Counter](https://docs.python.org/3/library/collections.html#collections.Counter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "031VQW14JqQp",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def get_bag_of_words(sentence):\n",
        "    \"\"\"\n",
        "    Tokenizes the sentence into separate words.\n",
        "    Args:\n",
        "        sentence: string containing a sentence\n",
        "\n",
        "    Returns: Counter of the tokens of the input sentence\n",
        "\n",
        "    \"\"\"\n",
        "    ### YOUR CODE HERE\n",
        "    tokens = sentence.split()\n",
        "    word_counts = Counter(tokens)\n",
        "    return word_counts\n",
        "    ### YOUR CODE ENDS HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "yeNIKx6KJqQp",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Let's test the function you just wrote:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "b8Vrbs3sJqQp",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nice new signage are you not concerned by beatlemania style hysterical crowds crongregating on you\n",
            "Counter({'you': 2, 'nice': 1, 'new': 1, 'signage': 1, 'are': 1, 'not': 1, 'concerned': 1, 'by': 1, 'beatlemania': 1, 'style': 1, 'hysterical': 1, 'crowds': 1, 'crongregating': 1, 'on': 1})\n"
          ]
        }
      ],
      "source": [
        "sentence = tweet_ds['train'][0]['clean']\n",
        "print(sentence)\n",
        "\n",
        "sentence_bow = get_bag_of_words(sentence)\n",
        "print(sentence_bow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "IeMnM-WHJqQp",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "We will also be interested in the word count of the whole dataset (or it's subset). The next function will combine word counts specified as a list into a single Counter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "wpRuUYOCJqQp",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<a name='e3'></a>\n",
        "### Exercise 3 (points 2)\n",
        "\n",
        "Fill in the following function to return a counter combining the provided list of counters. If the word is present in multiple counters the result should sum all the occurrences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Cu5qzL8NJqQp",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def aggregate_bags_of_words(bags_of_words):\n",
        "    \"\"\"\n",
        "    Aggregates the provided list containing bags-of-words\n",
        "    Args:\n",
        "        bags_of_words: list of Counters\n",
        "\n",
        "    Returns: Counter of the tokens of all bags-of-words\n",
        "\n",
        "    \"\"\"\n",
        "       # Initialize an empty Counter to aggregate the tokens\n",
        "    aggregated_bag = Counter()\n",
        "\n",
        "    # Iterate over each bag of words\n",
        "    for bag in bags_of_words:\n",
        "        # Update the aggregated bag with the counts from the current bag\n",
        "        aggregated_bag.update(bag)\n",
        "\n",
        "    # Return the aggregated bag\n",
        "    return aggregated_bag\n",
        "    ### YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "cFD0ftRUJqQp",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "The next function will calculate word counts of each cleaned sentence of the dataset split and then combine them into a single Counter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "mChR7aFaJqQp",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<a name='e4'></a>\n",
        "### Exercise 4 (points 2)\n",
        "\n",
        "Fill in the following function to obtain the counter representing the whole dataset. Use the function ```aggregate_bags_of_words()``` you implemented in the previous exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "H8P_tkpHJqQp",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def get_dataset_word_count(dataset_split):\n",
        "    \"\"\"\n",
        "    Creates a word count (a Counter) for the dataset split.\n",
        "    Args:\n",
        "        dataset_split: a dataset split\n",
        "\n",
        "    Returns: Counter of the tokens of the whole split\n",
        "\n",
        "    \"\"\"\n",
        "    temp = []\n",
        "    for i in range(len(dataset_split)):\n",
        "        temp.append(get_bag_of_words(dataset_split[i]['clean']))\n",
        "\n",
        "    return aggregate_bags_of_words(temp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "vXQikOKLJqQp",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Here are the word counts of the `train` and `validation` subsets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "nQRv43qGJqQp",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "train_word_count = get_dataset_word_count(tweet_ds['train'])\n",
        "validation_word_count = get_dataset_word_count(tweet_ds['validation'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "3dnzdCd6JqQp",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Let us explore them a bit more"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "JwXQOvRoJqQp",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train word count: 16173\n",
            "Most common words:\n",
            "[('the', 5504), ('to', 4783), ('a', 3923), ('you', 3361), ('and', 3202), ('of', 2877), ('in', 2560), ('is', 2051)]\n",
            "Least common words:\n",
            "[('gangbang', 1), ('leaf', 1), ('generalshame', 1), ('happiest', 1), ('birthdays', 1), ('oooohhhh', 1), ('kennedys', 1), ('picwhy', 1)]\n",
            "validation word count: 4716\n",
            "[('the', 758), ('to', 613), ('you', 518), ('a', 498), ('and', 420), ('of', 349), ('in', 330), ('is', 267), ('i', 226), ('that', 199)]\n",
            "Most common words:\n",
            "[('the', 758), ('to', 613), ('you', 518), ('a', 498), ('and', 420), ('of', 349), ('in', 330), ('is', 267)]\n",
            "Least common words:\n",
            "[('voting', 1), ('learned', 1), ('independence', 1), ('infeuenc', 1), ('scale', 1), ('delhihawkers', 1), ('menaceillegal', 1), ('constructionsappeasement', 1)]\n"
          ]
        }
      ],
      "source": [
        "print(f'train word count: {len(train_word_count)}')\n",
        "print('Most common words:')\n",
        "print(train_word_count.most_common(8))\n",
        "\n",
        "print('Least common words:')\n",
        "print(train_word_count.most_common()[-8:])\n",
        "\n",
        "print(f'validation word count: {len(validation_word_count)}')\n",
        "print(validation_word_count.most_common(10))\n",
        "\n",
        "print('Most common words:')\n",
        "print(validation_word_count.most_common(8))\n",
        "\n",
        "print('Least common words:')\n",
        "print(validation_word_count.most_common()[-8:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "S6hfCe7EJqQp",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "We can also plot the counts of the words. You can check the [Power law](https://en.wikipedia.org/wiki/Power_law) if you are more interested."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "29gZr1jvJqQp",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.loglog([val for word,val in train_word_count.most_common()])\n",
        "plt.loglog([val for word,val in validation_word_count.most_common()])\n",
        "plt.xlabel('rank')\n",
        "plt.ylabel('count')\n",
        "plt.legend(['training set','validation set']);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "X-ConZy-JqQq",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Now let's look at the sentences that are marked as hate and non-hate. We can use the method ```filter``` to get two datasets containing only the sentences labeled the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5XhwCJaIJqQq",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "non_hate_ds = tweet_ds['train'].filter(lambda ex: ex['label'] == 0)\n",
        "hate_ds = tweet_ds['train'].filter(lambda ex: ex['label'] == 1)\n",
        "\n",
        "non_hate_word_count = get_dataset_word_count(non_hate_ds)\n",
        "hate_word_count = get_dataset_word_count(hate_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "rodrUyuRJqQq",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Are there any words that are present in one but not in the other?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg460T13OE9R"
      },
      "source": [
        "<a name='e5'></a>\n",
        "### Exercise 5 (points 5)\n",
        "\n",
        "Fill in the following function to return (a Counter of) words that are not in the provided vocabulary. The vocabulary is in the form of a list of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "8WKJ3eApJqQq",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def get_oov_word_count(word_count, vocab):\n",
        "    \"\"\"\n",
        "    Finds the words that are not in the provided vocabulary.\n",
        "    Args:\n",
        "        word_count: Counter of the words\n",
        "        vocab: a list of words in the vocabulary\n",
        "\n",
        "    Returns: a Counter of the words that are outside the vocabulary\n",
        "\n",
        "    \"\"\"\n",
        "    ### YOUR CODE HERE\n",
        "    oov_words = Counter()\n",
        "    for word, count in word_count.items():\n",
        "        if word not in vocab:\n",
        "            oov_words[word] = count\n",
        "    return oov_words\n",
        "\n",
        "    ### YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "0MTLichSJqQq",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "only non-hate word count: 6998\n",
            "only hate word count: 4050\n",
            "Most common non-hate words:\n",
            "[('catholic', 19), ('palestinian', 18), ('blog', 18), ('rebuild', 17), ('advertising', 14), ('classes', 14), ('film', 12), ('sessions', 12), ('separating', 12), ('online', 12)]\n",
            "Most common hate words:\n",
            "[('cheated', 9), ('africans', 8), ('hystericalyoure', 8), ('dreamer', 7), ('lazy', 7), ('invaded', 6), ('overpopulation', 6), ('fame', 6), ('drivers', 6), ('michelle', 6)]\n"
          ]
        }
      ],
      "source": [
        "non_hate_only_word_count = get_oov_word_count(non_hate_word_count, hate_word_count.keys())\n",
        "hate_only_word_count = get_oov_word_count(hate_word_count, non_hate_word_count.keys())\n",
        "\n",
        "print(f'only non-hate word count: {len(non_hate_only_word_count)}')\n",
        "print(f'only hate word count: {len(hate_only_word_count)}')\n",
        "\n",
        "print('Most common non-hate words:')\n",
        "print(non_hate_only_word_count.most_common(10))\n",
        "\n",
        "print('Most common hate words:')\n",
        "print(hate_only_word_count.most_common(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "v8NyFBOAJqQq",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Often some words are used rarely in a specific context. We can remove them based on a threshold (a minimum number of occurrences that are required to keep the word in).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgffQ6JyOL-C"
      },
      "source": [
        "<a name='e6'></a>\n",
        "### Exercise 6 (points 4)\n",
        "\n",
        "Fill in the following function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "a6JKSPwUJqQq",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def prune_word_count(word_count, threshold):\n",
        "    \"\"\"\n",
        "    Removes words from the word counter that occur less than the specified threshold\n",
        "    Args:\n",
        "        word_count: Counter with the word counts\n",
        "        threshold: a minimum number of occurrences\n",
        "\n",
        "    Returns: a Counter with frequent words\n",
        "\n",
        "    \"\"\"\n",
        "    ### YOUR CODE HERE\n",
        "\n",
        "    pruned_word_count = Counter({word: count for word, count in word_count.items() if count >= threshold})\n",
        "    return pruned_word_count\n",
        "\n",
        "\n",
        "    ### YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "MQAjjWtMJqQr",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "pruned_word_count = prune_word_count(train_word_count, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "u011BldCJqQr",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train word count: 16173\n",
            "pruned word count: 4006\n",
            "Most common words:\n",
            "[('the', 5504), ('to', 4783), ('a', 3923), ('you', 3361), ('and', 3202), ('of', 2877), ('in', 2560), ('is', 2051)]\n",
            "Least common words:\n",
            "[('harper', 4), ('I', 4), (\"don't\", 4), ('ms', 4), ('nerve', 4), ('jungle', 4), ('updates', 4), ('complex', 4)]\n"
          ]
        }
      ],
      "source": [
        "print(f'train word count: {len(train_word_count)}')\n",
        "print(f'pruned word count: {len(pruned_word_count)}')\n",
        "\n",
        "print('Most common words:')\n",
        "print(pruned_word_count.most_common(8))\n",
        "\n",
        "print('Least common words:')\n",
        "print(pruned_word_count.most_common()[-8:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "U3UfnuzdJqQr",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Now (finally) we will construct a vocabulary. It will be a list of words (sorted alphabetically) that are present in the pruned word counter of the training subset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "sOoJT1qPJqQr",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size: 4006\n",
            "['0', '1', '10', '100', '10000', '100000', '11', '12', '13', '135']\n"
          ]
        }
      ],
      "source": [
        "vocab = sorted(pruned_word_count.keys())\n",
        "vocab_size = len(vocab)\n",
        "print(f'Vocab size: {vocab_size}')\n",
        "print(vocab[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4UvEoGZMfMO"
      },
      "source": [
        "<a name='4'></a>\n",
        "## 4. Build a Handcrafted Classifier\n",
        "\n",
        "Recall from the lectures that a classification model, is practically a function that maps the input text to the output classes. In our case, the output classes are 2: non-hate (return ```0```) or hate (return ```1```). In this section, we will build such a function, based on a rule-based system.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a07uoNmv8jS"
      },
      "source": [
        "<a name='e7'></a>\n",
        "### Exercise 7 (points 5)\n",
        "\n",
        "In order to device such simple classification rules, we will use analysis we conducted previously about which words are frequent in which class (i.e. non-hate vs. hate). Try to figure out an algorithm (based on if-else statements, aka rules) that would detect hate speech. Code the algorithm in the following function. You should not try to be exhaustive here and come up with the perfect rules (we are not sure they exist) but it's important to reflect (at the next exercise) on the trade-offs of implementing a rule-based classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "5IGmwIbSNLJc"
      },
      "outputs": [],
      "source": [
        "def detect_hate(text):\n",
        "  bow = get_bag_of_words(text)\n",
        "\n",
        "  ### YOUR CODE HERE\n",
        "\n",
        "  if 'whore' in bow or 'kill' in bow or '#buildthatwall' in bow or 'slut' in bow or 'nigger' in bow or 'nigga' in bow or 'faggot' in bow or 'cuck' in bow or 'retard' in bow or 'terrorist' in bow or 'hoe' in bow or 'hoes' in bow or 'ban' in bow or 'banned' in bow :\n",
        "    return 1                                                      \n",
        "  else:                                                         \n",
        "    return 0      \n",
        "\n",
        "  ### YOUR CODE ENDS HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6oSQTXqOEHi"
      },
      "source": [
        "The next code block will apply your function to the dataset to obtain the predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "2zl10-5IOLa9"
      },
      "outputs": [],
      "source": [
        "def apply_hate_speech_detector(example):\n",
        "  text = example['clean']\n",
        "  predicted_label = detect_hate(text)\n",
        "  example['predicted_label'] = predicted_label\n",
        "  return example\n",
        "\n",
        "# apply the function to all examples\n",
        "predictions = tweet_ds.map(apply_hate_speech_detector)\n",
        "# remove columns other than 'predicted_label'\n",
        "predictions = predictions.remove_columns(tweet_ds['train'].column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIgC2sP7Onx-"
      },
      "source": [
        "Here we create a small function to print the F1 score for predictions of the model. We will use it shortly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "iixOOsbJJqQs",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
        "\n",
        "def print_scores(y, prediction, report=False):\n",
        "    print('f1:', f1_score(y, prediction, average='macro'))\n",
        "    if report:\n",
        "        print(classification_report(y, prediction))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGbPx_7mnjQp"
      },
      "source": [
        "Let's retrieve the predictions for each subset (train, validation, test)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Pq83XswoPTvd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['predicted_label'],\n",
            "        num_rows: 8993\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['predicted_label'],\n",
            "        num_rows: 2970\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['predicted_label'],\n",
            "        num_rows: 999\n",
            "    })\n",
            "})\n",
            "train\n",
            "(8993,)\n",
            "(8993,)\n",
            "valid\n",
            "(999,)\n",
            "(999,)\n",
            "test\n",
            "(2970,)\n",
            "(2970,)\n"
          ]
        }
      ],
      "source": [
        "print(predictions)\n",
        "\n",
        "train_prediction = predictions['train']['predicted_label']\n",
        "y_train = tweet_ds['train']['label']\n",
        "print('train')\n",
        "print(train_prediction.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "valid_prediction = predictions['validation']['predicted_label']\n",
        "y_valid = tweet_ds['validation']['label']\n",
        "print('valid')\n",
        "print(valid_prediction.shape)\n",
        "print(y_valid.shape)\n",
        "\n",
        "test_prediction = predictions['test']['predicted_label']\n",
        "y_test = tweet_ds['test']['label']\n",
        "print('test')\n",
        "print(test_prediction.shape)\n",
        "print(y_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwJzn9CInrY-"
      },
      "source": [
        "Here we will print the F1 scores of your predictions for each subset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "PucCvtHGU477"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training score:\n",
            "f1: 0.498407900391982\n",
            "validation score:\n",
            "f1: 0.4871779487117949\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.97      0.74       572\n",
            "           1       0.78      0.14      0.23       427\n",
            "\n",
            "    accuracy                           0.61       999\n",
            "   macro avg       0.69      0.55      0.49       999\n",
            "weighted avg       0.68      0.61      0.52       999\n",
            "\n",
            "test score:\n",
            "f1: 0.5915586001687614\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.82      0.72      1718\n",
            "           1       0.60      0.37      0.46      1252\n",
            "\n",
            "    accuracy                           0.63      2970\n",
            "   macro avg       0.62      0.60      0.59      2970\n",
            "weighted avg       0.63      0.63      0.61      2970\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('training score:')\n",
        "print_scores(y_train, train_prediction)\n",
        "print('validation score:')\n",
        "print_scores(y_valid, valid_prediction, report=True)\n",
        "print('test score:')\n",
        "print_scores(y_test, test_prediction, report=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HdMM9LiZ14G"
      },
      "source": [
        "Let us also look at some examples with their predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "52a8ewRoZ1fY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "@user , you are correct that Reid certainly is a weasel. Sadly, we've got our own weasels; @user Sen McConnell & @user .The corrupt Mueller investigation w/be STOPPED if those 3 did their jobs.#MAGA #KAG #POTUS #Trump #NEWS #VoteRed #NoDACA #USA\n",
            "Predicted label: 0\n",
            "True label: 0\n",
            "\n",
            "Whoever just unfollowed me you a bitch\n",
            "Predicted label: 0\n",
            "True label: 1\n",
            "\n",
            "@user @user Those People Invaded Us!!! They DO NOT BELING HERE & HAVE NO RIGHTS! Its #AmericaFIRST! Open Your House To Them If Your That IGNORANT! & Yes Im A #Christian Too! #NODACA!\n",
            "Predicted label: 0\n",
            "True label: 1\n",
            "\n",
            "stop JUDGING bitches by there cover, jus cuz she bad don't mean she's a catch shawdy could be a whore ðŸ‘€ das opposite of a keeper\n",
            "Predicted label: 1\n",
            "True label: 1\n",
            "\n",
            "how about i knock heads off and send them gift wrapped to your moms house you dumb raggedy bird bitch ass hoes\n",
            "Predicted label: 1\n",
            "True label: 1\n",
            "\n",
            "@user @user @user Always #NoDACA.I AM BORN IN #USA AND #USA FIRST.\n",
            "Predicted label: 0\n",
            "True label: 0\n",
            "\n",
            "@user friends ? that's my BLOOD brother bitch .. he in yo city !! i'm hours away . ill be through there tomoâ€¦\n",
            "Predicted label: 0\n",
            "True label: 0\n",
            "\n",
            "@user @user Muslims attacked US on 9/11, 3000 killed. Subsequently we allowed more Muslims in our country?!?Does this make any sense to you?You can thank Obama, his liberals minions, RINO's & political correctness. #MuslimBan #BanIslam #TravelBan #DrainTheDeepState #DeportThemAll\n",
            "Predicted label: 0\n",
            "True label: 1\n",
            "\n",
            "@user @user Like he ever kept out any threats. He's lying as usual. #BuildThatWall\n",
            "Predicted label: 0\n",
            "True label: 0\n",
            "\n",
            "#germany deserves to called #cuck/land they allow in #isis fighters as #Refugees even if they raped underaged girls. this #yazidi girl is scared of this and she is forced to see the face of her rapist.#refugeesnotwelcome should apply to these scum\n",
            "Predicted label: 0\n",
            "True label: 1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    example = tweet_ds['test'][i]\n",
        "    print(example['text'])\n",
        "    print(f'Predicted label: {test_prediction[i]}')\n",
        "    print(f'True label: {y_test[i]}')\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bdJ8hCqXJpx"
      },
      "source": [
        "<a name='e8'></a>\n",
        "### Exercise 8 (points 5)\n",
        "\n",
        "In this section, discuss the results you obtained. Potentially, try to improve your algorithm based on the results (e.g. do you have many false positives? or false negatives?). Make sure to comment on the trade-offs of such a simplified system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ki0sOBPbaQjJ"
      },
      "source": [
        "The test and validation scores of this algorithm seem to be very similar. That is a good sign, as it demonstrates consistency.\n",
        "Initially, I made an error with the if statement initially, which meant that everything was classified as hate-speech. It was interesting that the accuracy rates were higher with that. But this situation was also resulting in a lot of false negatives. The recall rates in the validation set seem to yield odd results. WIth one closing in on 1 and the other to 0. This is not observeed in the test set. \n",
        "\n",
        "With hate-speech sentimental analysis, it is important to realise that that words need to be taken in context. For example, the word 'bitch' can be used in both ways. This might be a direct consequence of use of slang. Furthermore, words are not in their vector form. Which means 'hoe' and 'hoeing' would be treat differently. Also, with the use of Hashtags, the words are often compressed with no use of space. To improve the model, one can also take into account more words that would be indicative of hate-speech. This algorithm takes very limited words which popped up at the top of my head. \n",
        "\n",
        "I searched up some strategies via chatGPT and I came across a ranking system to work with contexts. It might make sense to take negations into account but it is difficult to work with in such a simplistic model. The ranking system would cancel the use of hate words if negation was found in a sentence. This technique would most importantly help us get rid of false positives/negatives. However, if implemented incorrectly, they could turn over the entire model. \n",
        "\n",
        "It is also essential that we take into account with that the bias of the programmer into account. One may consider certain political statements to be hate speech while the other may agree with them. This might make it difficult. This is why we must either work in teams or trust the developer to limit their biases. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "O685cntcJqQr",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<a name='5'></a>\n",
        "## 5. Build Bag-of-Words\n",
        "\n",
        "Having a vocabulary is important for indexing every word in our corpus.\n",
        "To represent a document as a vector however, we need more than just indexing, such as a vector space that represents the words:\n",
        "\n",
        "* Bag-of-Words model: A single document can be considered as a bag of words and how many times each word occured, without caring about the order of the words. The word occurence counting is also called term frequency. You can think if this as a vector over all of the vocabulary where the entries are how many times that term has occured.\n",
        "\n",
        "* TF-IDF: term frequencyâ€“inverse document frequency diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Mu0gxIhaJqQr",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<a name='e9'></a>\n",
        "### Exercise 9 (points 5)\n",
        "\n",
        "Let us build the bag-of-words for the whole dataset. We will only include the words that are present in our vocabulary that we created in the previous section. Words that are missing from the vocabulary will be replaced with the <unknown> token. For training we need to convert the counter into a numpy array [https://numpy.org/doc/stable/reference/generated/numpy.array.html](https://numpy.org/doc/stable/reference/generated/numpy.array.html). The array will be of the size of the vocabulary (plus 1 for the <unknown> token). Each element will correspond to the word in our vocabulary and contain the number of occurrences of this word in a particular sentence. Fill in the following function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "wsRwR-pKJqQr",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def calculate_bag_of_words(example):\n",
        "    \"\"\"\n",
        "    Calculates the word count and encodes it as a numpy array of the size equal to the vocab\n",
        "    Args:\n",
        "        example: an example from the Dataset\n",
        "\n",
        "    Returns: updated example with 'bow' column\n",
        "\n",
        "    \"\"\"\n",
        "    text = example['clean']\n",
        "    bow = get_bag_of_words(text)\n",
        "    # create numpy array with the size of the vocab plus 1 for the unknown token\n",
        "    bow_numpy = np.zeros((vocab_size + 1), dtype=int)\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "\n",
        "    for word, count in bow.items():\n",
        "        if word in vocab:\n",
        "            bow_numpy[vocab.index(word)] = count\n",
        "        else:\n",
        "            bow_numpy[vocab_size] += count\n",
        "\n",
        "    ### YOUR CODE ENDS HERE\n",
        "    example['bow'] = bow_numpy\n",
        "    return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "3_eN_JHcJqQr",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'text': '@user nice new signage. Are you not concerned by Beatlemania -style hysterical crowds crongregating on youâ€¦', 'label': 0, 'clean': 'nice new signage are you not concerned by beatlemania style hysterical crowds crongregating on you', 'bow': array([0, 0, 0, ..., 0, 0, 4])}\n"
          ]
        }
      ],
      "source": [
        "print(calculate_bag_of_words(tweet_ds['train'][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Ldq19LLiJqQr",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label', 'clean', 'bow'],\n",
            "        num_rows: 8993\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label', 'clean', 'bow'],\n",
            "        num_rows: 2970\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text', 'label', 'clean', 'bow'],\n",
            "        num_rows: 999\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "tweet_ds = tweet_ds.map(calculate_bag_of_words)\n",
        "\n",
        "print(tweet_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "PXRJV4B8JqQs",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<a name='6'></a>\n",
        "## 6. Bag-of-Words with Naive Bayes\n",
        "\n",
        "Our first classification algorithm will be the Naive-Bayes algorithm. Naive Bayes is a generative classification algorithm: the model assigns class labels to the input text, which is represented as a vectors of feature values (this will be our bag-of-words).\n",
        "\n",
        "If you need a refresher on the method, please revisit the lecture slides."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5K7JlqjpvTV"
      },
      "source": [
        "We can start by extracting the features and labels from our dataset. We will use it to train the classifier (on 'train' subset) and evaluate it after training (on 'valid' and 'test' subsets).\n",
        "\n",
        "The dataset will return the features as numpy arrays. We can inspect their shapes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Gw1DedUPJqQr",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(8993, 4007)\n",
            "(8993,)\n",
            "(999, 4007)\n",
            "(999,)\n",
            "(2970, 4007)\n",
            "(2970,)\n"
          ]
        }
      ],
      "source": [
        "X_train = tweet_ds['train']['bow']\n",
        "y_train = tweet_ds['train']['label']\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "X_valid = tweet_ds['validation']['bow']\n",
        "y_valid = tweet_ds['validation']['label']\n",
        "\n",
        "print(X_valid.shape)\n",
        "print(y_valid.shape)\n",
        "\n",
        "X_test = tweet_ds['test']['bow']\n",
        "y_test = tweet_ds['test']['label']\n",
        "\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PbyEjcSp706"
      },
      "source": [
        "The scikit-learn library also contains the implementation of Naive Bayes. You will use it in the next exercise. Here is the documentation [https://scikit-learn.org/stable/modules/naive_bayes.html](https://scikit-learn.org/stable/modules/naive_bayes.html). The model is trained by calling the ```fit()``` method. It receives the features of the training examples and their labels. Depending on your machine, the training might take from several seconds to up to a minute or two."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "VaNamSHYJqQt",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "GaussianNB()"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "nb_classifier = GaussianNB()\n",
        "nb_classifier.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBHHQrwhOy9l"
      },
      "source": [
        "We can use the trained classifier to make predictions. The method ```predict()``` will output the predictions made by the model. We can score them and print the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "GBs-atNhJqQt",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training score:\n",
            "f1: 0.635906392733487\n",
            "validation score:\n",
            "f1: 0.5577441077441078\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.34      0.47       572\n",
            "           1       0.50      0.89      0.64       427\n",
            "\n",
            "    accuracy                           0.57       999\n",
            "   macro avg       0.65      0.61      0.56       999\n",
            "weighted avg       0.68      0.57      0.55       999\n",
            "\n"
          ]
        }
      ],
      "source": [
        "nb_train_prediction = nb_classifier.predict(X_train)\n",
        "nb_valid_prediction = nb_classifier.predict(X_valid)\n",
        "\n",
        "print('training score:')\n",
        "print_scores(y_train, nb_train_prediction)\n",
        "print('validation score:')\n",
        "print_scores(y_valid, nb_valid_prediction, report=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4xSxoIfpdAC"
      },
      "source": [
        "Let's do the same for the test subset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "63snGggAJqQt",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training score:\n",
            "f1: 0.635906392733487\n",
            "validation score:\n",
            "f1: 0.5577441077441078\n",
            "test score:\n",
            "f1: 0.40357458669876023\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.12      0.21      1718\n",
            "           1       0.44      0.93      0.59      1252\n",
            "\n",
            "    accuracy                           0.46      2970\n",
            "   macro avg       0.58      0.53      0.40      2970\n",
            "weighted avg       0.60      0.46      0.37      2970\n",
            "\n"
          ]
        }
      ],
      "source": [
        "nb_test_prediction = nb_classifier.predict(X_test)\n",
        "print('training score:')\n",
        "print_scores(y_train, nb_train_prediction)\n",
        "print('validation score:')\n",
        "print_scores(y_valid, nb_valid_prediction)\n",
        "print('test score:')\n",
        "print_scores(y_test, nb_test_prediction, report=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "p3Esqe7rJqQt",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<a name='e10'></a>\n",
        "### Exercise 10 (points 10)\n",
        "\n",
        "Analyze and comment on the results. Compare to the previous results. Do you see anything unexpected? Provide explanations if so."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qy2whYbSqab6"
      },
      "source": [
        "- **Overfitting**: The model performs well on the training data but poorly on unseen validation and test data, indicating it is overfitting. It is possible that the model is reliant on the few words that have been picked upon for hate speeech. A lot of words that were not used may have resulted in the inaccurate classifation. \n",
        "- **Bias Towards Predicting Hate Speech**: What is unexpected is how the model exhibits good recall but low precision for detecting hate speech. It successfully identifies most hate speech but also mislabels a lot of non-hate speech as hatee speech (low precision) - this can be a contextual issue that has been mentioned in Q8. Words surrounding the 'hate speech' need to be taken into consideration to be able to remove this bsia. This is particularly problematic as it could lead to over-censorship in real life situations.\n",
        "- **Poor Detection of Non-Hate Speech**: The very low recall for non-hate speech in the test dataset indicates that the model frequently misses identifying non-hate speech correctly, mistaking it for hate speech. slang is another factor that mucst be taken into consideration. The majority of users on twitter tend to be younger. So slang constantly eveolves and words may not represent their dictionary meanings. While this was not 'unexpected', this is something that was not taken into consideration initally.\n",
        "- **Optimize Training Approach**: Apply regularization techniques to reduce overfitting and ensure the model generalizes well across different datasets. Simplifying the model might also help in improving generalizability. We could do a cost analysis to determine the weights or impacts of false positives/negatics. With sentiments being taken into account, it is importantt to note that incorrect analysis could change tides and cause even riots.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "7gS-N5paJqQr",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<a name='7'></a>\n",
        "## 7. Bag-of-Words with Logistic Regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_Phlb4uNuFH"
      },
      "source": [
        "In this section, we will apply a different classification algorithm, namely Logistic Regression. If you need a refresher about Logistic Regression, make sure to revisit the lecture slides.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWtMVF8OODxb"
      },
      "source": [
        "You can check the documentation of the LogisticRegression here [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). The model is trained by calling the ```fit()``` method. It receives the features of the training examples and their labels. Depending on your machine, the training might take from several seconds to up to a minute or two."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "in-_9XlTJqQs",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<a name='e11'></a>\n",
        "### Exercise 11 (points 3)\n",
        "\n",
        "Train Logistic Regression on the bag-of-words features.\n",
        "First, instantiate the classifier (visit the documentation to find the parameters of the constructor [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)). Next, train the classifier on the training data (```X_train``` and ```y_train```)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "-AeED0K0JqQs",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/wiksrivastava/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "LogisticRegression()"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr_classifier = LogisticRegression()\n",
        "\n",
        "lr_classifier.fit(X_train, y_train)\n",
        "\n",
        "#! why is the X capitalised and y not?? That made me lose some time haha....\n",
        "\n",
        "\n",
        "### YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvD7nlBdQefX"
      },
      "source": [
        "Now, let's see how the model is performing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "nxak5F5QJqQs",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training score:\n",
            "f1: 0.8731330918164347\n",
            "validation score:\n",
            "f1: 0.672989011964887\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.75      0.73       572\n",
            "           1       0.64      0.60      0.62       427\n",
            "\n",
            "    accuracy                           0.68       999\n",
            "   macro avg       0.68      0.67      0.67       999\n",
            "weighted avg       0.68      0.68      0.68       999\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_prediction = lr_classifier.predict(X_train)\n",
        "valid_prediction = lr_classifier.predict(X_valid)\n",
        "\n",
        "print('training score:')\n",
        "print_scores(y_train, train_prediction)\n",
        "print('validation score:')\n",
        "print_scores(y_valid, valid_prediction, report=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAhZA3gEPRBk"
      },
      "source": [
        "Let's do the same for the test subset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "F1JKt2YNJqQs",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training score:\n",
            "f1: 0.8731330918164347\n",
            "validation score:\n",
            "f1: 0.672989011964887\n",
            "test score:\n",
            "f1: 0.5871132377711603\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.42      0.54      1718\n",
            "           1       0.51      0.83      0.63      1252\n",
            "\n",
            "    accuracy                           0.59      2970\n",
            "   macro avg       0.64      0.62      0.59      2970\n",
            "weighted avg       0.66      0.59      0.58      2970\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_prediction = lr_classifier.predict(X_test)\n",
        "\n",
        "print('training score:')\n",
        "print_scores(y_train, train_prediction)\n",
        "print('validation score:')\n",
        "print_scores(y_valid, valid_prediction)\n",
        "print('test score:')\n",
        "print_scores(y_test, test_prediction, report=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "8v-bU9tbsxu3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2970,)\n"
          ]
        }
      ],
      "source": [
        "print(test_prediction.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxwawpz8Paia"
      },
      "source": [
        "You can also inspect the predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "-4WAKgEUJqQs",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "@user , you are correct that Reid certainly is a weasel. Sadly, we've got our own weasels; @user Sen McConnell & @user .The corrupt Mueller investigation w/be STOPPED if those 3 did their jobs.#MAGA #KAG #POTUS #Trump #NEWS #VoteRed #NoDACA #USA\n",
            "Predicted label: 1\n",
            "True label: 0\n",
            "\n",
            "Whoever just unfollowed me you a bitch\n",
            "Predicted label: 0\n",
            "True label: 1\n",
            "\n",
            "@user @user Those People Invaded Us!!! They DO NOT BELING HERE & HAVE NO RIGHTS! Its #AmericaFIRST! Open Your House To Them If Your That IGNORANT! & Yes Im A #Christian Too! #NODACA!\n",
            "Predicted label: 1\n",
            "True label: 1\n",
            "\n",
            "stop JUDGING bitches by there cover, jus cuz she bad don't mean she's a catch shawdy could be a whore ðŸ‘€ das opposite of a keeper\n",
            "Predicted label: 1\n",
            "True label: 1\n",
            "\n",
            "how about i knock heads off and send them gift wrapped to your moms house you dumb raggedy bird bitch ass hoes\n",
            "Predicted label: 1\n",
            "True label: 1\n",
            "\n",
            "@user @user @user Always #NoDACA.I AM BORN IN #USA AND #USA FIRST.\n",
            "Predicted label: 0\n",
            "True label: 0\n",
            "\n",
            "@user friends ? that's my BLOOD brother bitch .. he in yo city !! i'm hours away . ill be through there tomoâ€¦\n",
            "Predicted label: 0\n",
            "True label: 0\n",
            "\n",
            "@user @user Muslims attacked US on 9/11, 3000 killed. Subsequently we allowed more Muslims in our country?!?Does this make any sense to you?You can thank Obama, his liberals minions, RINO's & political correctness. #MuslimBan #BanIslam #TravelBan #DrainTheDeepState #DeportThemAll\n",
            "Predicted label: 0\n",
            "True label: 1\n",
            "\n",
            "@user @user Like he ever kept out any threats. He's lying as usual. #BuildThatWall\n",
            "Predicted label: 0\n",
            "True label: 0\n",
            "\n",
            "#germany deserves to called #cuck/land they allow in #isis fighters as #Refugees even if they raped underaged girls. this #yazidi girl is scared of this and she is forced to see the face of her rapist.#refugeesnotwelcome should apply to these scum\n",
            "Predicted label: 1\n",
            "True label: 1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    example = tweet_ds['test'][i]\n",
        "    print(example['text'])\n",
        "    print(f'Predicted label: {test_prediction[i]}')\n",
        "    print(f'True label: {y_test[i]}')\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "lBG9KQzjJqQs",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<a name='e12'></a>\n",
        "### Exercise 12 (points 10)\n",
        "\n",
        "Analyze and comment on the results. Do you see anything unexpected? Give explanations.\n",
        "\n",
        "Analyze at least 10 errors from the results (could be either false positives or negatives) and try to explain why they occured (e.g. was it because of some linguistic phenomenon that cannot be captured by the models, was it because of bad annotation etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmLbQfC9qcOV"
      },
      "source": [
        "**For the assignment, I will be assuming a false positive to be non-hate speech being detected as hate speech and hate-speech being detected as non-hate speech to be false negatives.**\n",
        "\n",
        "1. @user , you are correct that Reid certainly is a weasel. Sadly, we've got our own weasels; @user Sen McConnell & @user .The corrupt Mueller investigation w/be STOPPED if those 3 did their jobs.#MAGA #KAG #POTUS #Trump #NEWS #VoteRed #NoDACA #USA\n",
        "Predicted label: 1\n",
        "True label: 0\n",
        "\n",
        "**False positive** - This may have been calsiifeied as hate speech since similar terms and hashtags would often be calssified as hate-speech with the model that has been developed. This similarity causes ambiguity, where a logistic regession model would classify the likelihood to be higher for hate-speech than not for this tweet.\n",
        "\n",
        "2. Whoever just unfollowed me you a bitch\n",
        "Predicted label: 0\n",
        "True label: 1\n",
        "\n",
        "**False negative** - This classification is difficult. The text in itself is not directed towards a known entity. That makes hate speech detection difficult. It would be possible to get a better idea if the history of the user was taken into account but that would be time and energy intenseive.\n",
        "\n",
        "3. Too many immigrants unlicensed to kill #IllegalAliens #IllegalImmigration#Transportation #BuildThatWall @user @user\n",
        "Predicted label: 0\n",
        "True label: 1\n",
        "\n",
        "**False negative** None of the words that were used in the initial classifier have been used in this tweet. Furthermore, a lot of words that may otherwise help with the ininital classifier are hashtagged. These are not read the same way, making it difficult for the model to make correct judgements.\n",
        "\n",
        "4. a whole BOX of oreos bitch\n",
        "Predicted label: 1\n",
        "True label: 0\n",
        "\n",
        "**False positive** - This may have been calsiifeied as hate speech since the term 'bitch' would often be calssified as hate-speech with the model that has been developed. Words like these often cause confusions as slangs alter the meanings of such words. This model does not understand how slangs are used. \n",
        "\n",
        "5. @ the family that called me a cunt and a Bitch for their chip not working on their card ðŸ‘\n",
        "Predicted label: 1\n",
        "True label: 0\n",
        "\n",
        "**False positive** - This may have been calsiifeied as hate speech since the term 'bitch' would often be calssified as hate-speech with the model that has been developed. Words like these often cause confusions as slangs alter the meanings of such words. This model does not understand how slangs are used. \n",
        "\n",
        "6. Same shoes everyday but bitch I'm still swaggin' @user\n",
        "Predicted label: 1\n",
        "True label: 0\n",
        "\n",
        "**False positive** - This may have been calsiifeied as hate speech since the term 'bitch' would often be calssified as hate-speech with the model that has been developed. Words like these often cause confusions as slangs alter the meanings of such words. This model does not understand how slangs are used. \n",
        "\n",
        "7. \"Please don't call it \"\"rescue\"\" - it's human trafficking #portsclosed #sendthemback #benefitseekers\"\n",
        "Predicted label: 0\n",
        "True label: 1\n",
        "\n",
        "**False negative** None of the words that were used in the initial classifier have been used in this tweet. Furthermore, a lot of words that may otherwise help with the ininital classifier are hashtagged. These are not read the same way, making it difficult for the model to make correct judgements.\n",
        "\n",
        "8. @user @user That's what happens when you let in a bunch of savages from Africa riddled with diseases!!! #CloseTheBorders #SendThemBack\n",
        "Predicted label: 0\n",
        "True label: 1\n",
        "\n",
        "**False negative** None of the words that were used in the initial classifier have been used in this tweet. Furthermore, a lot of words that may otherwise help with the ininital classifier are hashtagged. These are not read the same way, making it difficult for the model to make correct judgements.\n",
        "\n",
        "9. @user BITCH DONT TEST ME\n",
        "Predicted label: 1\n",
        "True label: 0\n",
        "\n",
        "**False positive** - This may have been calsiifeied as hate speech since the term 'bitch' would often be calssified as hate-speech with the model that has been developed. Words like these often cause confusions as slangs alter the meanings of such words. This model does not understand how slangs are used. However, this could actually be hate speech. This is another situation where the historical twwets of the user would play a role in determining the reality. \n",
        "\n",
        "10. @user @user Muslims attacked US on 9/11, 3000 killed. Subsequently we allowed more Muslims in our country?!?Does this make any sense to you?You can thank Obama, his liberals minions, RINO's & political correctness. #MuslimBan #BanIslam #TravelBan #DrainTheDeepState #DeportThemAll\n",
        "Predicted label: 0\n",
        "True label: 1\n",
        "\n",
        "**False negative** None of the words that were used in the initial classifier have been used in this tweet. Furthermore, a lot of words that may otherwise help with the ininital classifier are hashtagged. These are not read the same way, making it difficult for the model to make correct judgements.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "KHtLxw-dJqQt",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<a name='8'></a>\n",
        "## 8. TF-IDF\n",
        "Comment on the above results and try to propose/come up with some improvements. Explore the possibilities of the bag-of-words and different variations (e.g. TF-IDF). Make sure that you comment on your results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "0o6QA5C6JqQt",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<a name='e13'></a>\n",
        "### Exercise 13 (points 5)\n",
        "\n",
        "Extract TF-IDF (or other, your choice) features for the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "VyVwDKbHJqQt",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "#### YOUR CODE HERE\n",
        "#! i sought the help of chatGPT with this question. I often ended up getting confused. Initially, my approach involved me counting the number of times a word was referenced.\n",
        "#! i believe that was incorrect as i had to identify the number of documents it came up in. This section of the module needs more work from me. i was also fortunate enough\n",
        "#! get some ideas from some of my colleagues. \n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def index_of_word(word):\n",
        "    try:\n",
        "        return vocab.index(word)\n",
        "    except ValueError:\n",
        "        return None\n",
        "\n",
        "def find_count_docs_word(word):\n",
        "    count = 0\n",
        "    word_index = index_of_word(word)\n",
        "    if word_index is None:\n",
        "        return 0\n",
        "    for document in X_train:\n",
        "        if document[word_index] > 0:\n",
        "            count += 1\n",
        "    return count\n",
        "\n",
        "def calculate_tf_idf(example):\n",
        "    text = example['clean']\n",
        "    words = text.split()  # Assuming 'clean' is a space-separated string of words\n",
        "    tf_idf_vector = np.zeros(len(vocab))\n",
        "    \n",
        "    word_counts = {word: words.count(word) for word in set(words)}\n",
        "    total_words = sum(word_counts.values())\n",
        "    \n",
        "    for word, count in word_counts.items():\n",
        "        if word in vocab:\n",
        "            tf = count / total_words\n",
        "            idf = math.log((len(X_train) + 1) / (find_count_docs_word(word) + 1))\n",
        "            tf_idf_vector[index_of_word(word)] = tf * idf\n",
        "    \n",
        "    example['tf_idf'] = tf_idf_vector\n",
        "    return example\n",
        "\n",
        "### YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsaZw6xws6v-"
      },
      "source": [
        "A result of the function being applied to a single example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "eSGFYtnTJqQt",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'text': '@user nice new signage. Are you not concerned by Beatlemania -style hysterical crowds crongregating on youâ€¦', 'label': 0, 'clean': 'nice new signage are you not concerned by beatlemania style hysterical crowds crongregating on you', 'bow': array([0, 0, 0, ..., 0, 0, 4]), 'tf_idf': array([0., 0., 0., ..., 0., 0., 0.])}\n"
          ]
        }
      ],
      "source": [
        "print(calculate_tf_idf(tweet_ds['train'][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfBWo0IctAJU"
      },
      "source": [
        "Apply the function to the whole dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "x-12IfEsJqQt",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label', 'clean', 'bow', 'tf_idf'],\n",
            "        num_rows: 8993\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label', 'clean', 'bow', 'tf_idf'],\n",
            "        num_rows: 2970\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text', 'label', 'clean', 'bow', 'tf_idf'],\n",
            "        num_rows: 999\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "tweet_ds = tweet_ds.map(calculate_tf_idf)\n",
        "\n",
        "print(tweet_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCMZfS9Htdhl"
      },
      "source": [
        "Extract the features for each subset. Labels (```y_train```, etc.) are the same as for the previous exercises."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "yUsD8bBgtoL9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(8993, 4006)\n",
            "(8993,)\n",
            "(999, 4006)\n",
            "(999,)\n",
            "(2970, 4006)\n",
            "(2970,)\n"
          ]
        }
      ],
      "source": [
        "X_train_tf_idf = tweet_ds['train']['tf_idf']\n",
        "\n",
        "print(X_train_tf_idf.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "X_valid_tf_idf = tweet_ds['validation']['tf_idf']\n",
        "\n",
        "print(X_valid_tf_idf.shape)\n",
        "print(y_valid.shape)\n",
        "\n",
        "X_test_tf_idf = tweet_ds['test']['tf_idf']\n",
        "\n",
        "print(X_test_tf_idf.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "3W2Gd_SNJqQt",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<a name='e14'></a>\n",
        "### Exercise 14 (points 10)\n",
        "\n",
        "Train a classifier of your choice on TF-IDF features. Evaluate it on the validation and test dataset and compare with the previous experiments. As usual, provide explanations and insights to your analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "dEkcLpotJqQt",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive Bayes Classifier Results->\n",
            "\n",
            "Validation Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.77      0.74       572\n",
            "           1       0.65      0.58      0.61       427\n",
            "\n",
            "    accuracy                           0.69       999\n",
            "   macro avg       0.68      0.67      0.68       999\n",
            "weighted avg       0.68      0.69      0.68       999\n",
            "\n",
            "Test Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.47      0.58      1718\n",
            "           1       0.52      0.79      0.63      1252\n",
            "\n",
            "    accuracy                           0.60      2970\n",
            "   macro avg       0.63      0.63      0.60      2970\n",
            "weighted avg       0.65      0.60      0.60      2970\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "naive_bayes_classifier = MultinomialNB()\n",
        "naive_bayes_classifier.fit(X_train_tf_idf, y_train)\n",
        "\n",
        "print(\"Naive Bayes Classifier Results->\")\n",
        "print(\"\")\n",
        "\n",
        "validation_predictions = naive_bayes_classifier.predict(X_valid_tf_idf)\n",
        "validation_report = classification_report(y_valid, validation_predictions)\n",
        "print(\"Validation Report:\\n\", validation_report)\n",
        "\n",
        "test_predictions = naive_bayes_classifier.predict(X_test_tf_idf)\n",
        "test_report = classification_report(y_test, test_predictions)\n",
        "print(\"Test Report:\\n\", test_report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qP05s4Ye3-La"
      },
      "source": [
        "When comparing the results from section 6, we can see that the accuracy rates for this set of data are significantly higher than that obtained for NB bag of words. This showcases that TF-IDF helps the model identify critical words. Previously, all words were treated equally, but now standard words tend to be given less imporatnce. This would help reduce the impact of stop-words as they have not been pre-processed. Such words would not contribute to the analyis. Correspondingly, the accuracy rates have improved with the implemntetation of the TF-IDF where the uniqueness/impact of a word is weighed and valued. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEgIPt-WrAR6"
      },
      "source": [
        "<a name='9'></a>\n",
        "## 9. Adding Handcrafted Features\n",
        "\n",
        "In this section you will use either Bag-of-Words or TF-IDF (whichever performed better) but with a twist. You will add handcrafted features designed by you: Recall that in lectures we discussed how we can expand Bag-of-Words for this purpose (e.g. to include features like # of positive words or # of negative words). They will be concatenated with BOW or TF-IDF numpy array and used to train the classifier of your choice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIjHBU8lr0Ks"
      },
      "source": [
        "<a name='e15'></a>\n",
        "### Exercise 15 (points 10)\n",
        "\n",
        "Fill in the following function and write your own features (e.g. number of hashtags in the example)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "IjHMNWmBrzcm"
      },
      "outputs": [],
      "source": [
        "def calculate_handcrafted_features(example):\n",
        "    \"\"\"\n",
        "    Calculates the handcrafted features for a given example\n",
        "    Args:\n",
        "        example: an example from the Dataset\n",
        "\n",
        "    Returns: updated example with 'handcrafted_features' column\n",
        "\n",
        "    \"\"\"\n",
        "    text = example['clean']\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "\n",
        "    # UPDATE this number to the number of features you intend to include\n",
        "    num_of_features = 2\n",
        "\n",
        "    #wordsTweet = len(text.split())\n",
        "    words = text.split()\n",
        "    #avgWordLength = sum(len(word) for word in words) / wordsTweet if wordsTweet > 0 else 0\n",
        "    #charactersTweet = len(text)\n",
        "\n",
        "    negative_specific_words = ['whore', 'kill', 'buildthatwall', 'slut', 'nigger', 'nigga', 'faggot', 'cuck', 'retard', 'terrorist', 'hoe', 'hoes', 'ban', 'banned']\n",
        "    negative_word_frequency = sum(1 for word in words if word in negative_specific_words)\n",
        "\n",
        "    negations_specific_words = ['not', 'no', 'nope', 'never']\n",
        "    negations_word_frequency = sum(1 for word in words if word in negations_specific_words)\n",
        "\n",
        "\n",
        "    # create numpy array of the size equal to the number of features\n",
        "    handcrafted_features = np.zeros(num_of_features, dtype=float)\n",
        "\n",
        "    # Assign computed features to the numpy array\n",
        "    #handcrafted_features[0] = wordsTweet\n",
        "    #handcrafted_features[1] = avgWordLength\n",
        "    #handcrafted_features[2] = charactersTweet\n",
        "    handcrafted_features[0] = negative_word_frequency\n",
        "    handcrafted_features[1] = negations_word_frequency\n",
        "\n",
        "    ### YOUR CODE ENDS HERE\n",
        "\n",
        "    example['handcrafted_features'] = handcrafted_features\n",
        "    return example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPYXFT-rtEex"
      },
      "source": [
        "Let's see the result of the function applied to a single example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "nO8FYdhdtENt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'text': '@user nice new signage. Are you not concerned by Beatlemania -style hysterical crowds crongregating on youâ€¦', 'label': 0, 'clean': 'nice new signage are you not concerned by beatlemania style hysterical crowds crongregating on you', 'bow': array([0, 0, 0, ..., 0, 0, 4]), 'tf_idf': array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), 'handcrafted_features': array([0., 1.])}\n"
          ]
        }
      ],
      "source": [
        "print(calculate_handcrafted_features(tweet_ds['train'][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7svF_Q1tMv4"
      },
      "source": [
        "Now, let's apply it to the whole dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "qTC1G_24tQ9X"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label', 'clean', 'bow', 'tf_idf', 'handcrafted_features'],\n",
            "        num_rows: 8993\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label', 'clean', 'bow', 'tf_idf', 'handcrafted_features'],\n",
            "        num_rows: 2970\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text', 'label', 'clean', 'bow', 'tf_idf', 'handcrafted_features'],\n",
            "        num_rows: 999\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "tweet_ds = tweet_ds.map(calculate_handcrafted_features)\n",
        "\n",
        "print(tweet_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8XT63Ant7u4"
      },
      "source": [
        "We can now extract the handcrafted features and combine with TF-IDF featuers (or any of your choice from previously calculated)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "-8-_kjBbuIAc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(8993, 4008)\n",
            "(8993,)\n",
            "(999, 4008)\n",
            "(999,)\n",
            "(2970, 4008)\n",
            "(2970,)\n"
          ]
        }
      ],
      "source": [
        "X_train_handcrafted = tweet_ds['train']['handcrafted_features']\n",
        "X_train_combined = np.concatenate((X_train_tf_idf, X_train_handcrafted), axis=-1)\n",
        "print(X_train_combined.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "X_valid_handcrafted = tweet_ds['validation']['handcrafted_features']\n",
        "X_valid_combined = np.concatenate((X_valid_tf_idf, X_valid_handcrafted), axis=-1)\n",
        "print(X_valid_combined.shape)\n",
        "print(y_valid.shape)\n",
        "\n",
        "X_test_handcrafted = tweet_ds['test']['handcrafted_features']\n",
        "X_test_combined = np.concatenate((X_test_tf_idf, X_test_handcrafted), axis=-1)\n",
        "print(X_test_combined.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdJIiZPQus9v"
      },
      "source": [
        "<a name='e16'></a>\n",
        "### Exercise 16 (points 10)\n",
        "\n",
        "Train the classifier of your choice (Naive Bayes, Logistic Regression, Neural Network, etc.) on the combined features. Evaluate it on the validation and test dataset and compare with the previous experiments. Did your handcrafted features improve the performance?\n",
        "\n",
        "This is an open ended question, meaning that you are free to implement different setups and judge the final performance. You do not need to include all your experiments but for every model you introduce make sure to check the performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "reykd5k8u9OP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naives Bayes:\n",
            "Validation Accuracy: 0.6866866866866866\n",
            "Test Accuracy: 0.6053872053872054\n",
            "\n",
            "Logistic Regression:\n",
            "Validation Accuracy: 0.6836836836836837\n",
            "Test Accuracy: 0.5973063973063973\n"
          ]
        }
      ],
      "source": [
        "#### YOUR CODE HERE\n",
        "#! Used chatGPT here to train the classifier models on combined features.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X_train_combined = np.concatenate((X_train_tf_idf, X_train_handcrafted), axis=-1)\n",
        "X_valid_combined = np.concatenate((X_valid_tf_idf, X_valid_handcrafted), axis=-1)\n",
        "X_test_combined = np.concatenate((X_test_tf_idf, X_test_handcrafted), axis=-1)\n",
        "\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train_combined, y_train)\n",
        "\n",
        "val_preds = clf.predict(X_valid_combined)\n",
        "\n",
        "print(\"Naives Bayes:\")\n",
        "\n",
        "val_accuracy = accuracy_score(y_valid, val_preds)\n",
        "print(\"Validation Accuracy:\", val_accuracy)\n",
        "\n",
        "test_preds = clf.predict(X_test_combined)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_preds)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "log_reg = LogisticRegression(max_iter=1000)\n",
        "log_reg.fit(X_train_combined, y_train)\n",
        "\n",
        "val_preds_log_reg = log_reg.predict(X_valid_combined)\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "print(\"Logistic Regression:\")\n",
        "\n",
        "val_accuracy_log_reg = accuracy_score(y_valid, val_preds_log_reg)\n",
        "print(\"Validation Accuracy:\", val_accuracy_log_reg)\n",
        "\n",
        "test_preds_log_reg = log_reg.predict(X_test_combined)\n",
        "\n",
        "test_accuracy_log_reg = accuracy_score(y_test, test_preds_log_reg)\n",
        "print(\"Test Accuracy:\", test_accuracy_log_reg)\n",
        "\n",
        "### MAKE SURE TO CHECK TRAIN/VALID/TEST SET PERFORMANCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ0dCxly5EuC"
      },
      "source": [
        "**For this question, the Naives Bayes and the Logistic Regression classifier models were developed for the new combined features.**\n",
        "\n",
        "**The first test involves handicraft features - number of words, number of characters and average word length. When comparing the results, it is difficult to see any noticiable differences in the accuracy rates between the Validatin and Test sets. The handicraft features did not improve the performance of the classifiers as they features do not happen to add any meaningful information to the models.**\n",
        "\n",
        "Results first test: Naives Bayes:\n",
        "Validation Accuracy: 0.6746746746746747\n",
        "Test Accuracy: 0.6016835016835017\n",
        "\n",
        "Logistic Regression:\n",
        "Validation Accuracy: 0.6796796796796797\n",
        "Test Accuracy: 0.5902356902356902\n",
        "\n",
        "**The second test involves handicraft features - number of words, number of characters, average word length, number of hate words used and number of negations used. When comparing the results, while insignifantly, the accuracy does increase. This increase is consistent across the two models. This motivates me to try some more combinations.** \n",
        "\n",
        "Naives Bayes:\n",
        "Validation Accuracy: 0.6746746746746747\n",
        "Test Accuracy: 0.604040404040404\n",
        "\n",
        "Logistic Regression:\n",
        "Validation Accuracy: 0.6796796796796797\n",
        "Test Accuracy: 0.5976430976430976\n",
        "\n",
        "**The third test involves handicraft features - number of hate words used and number of negations used. When comparing the results, while insignifantly, the accuracy does increase againt. This increase is consistent across the two models. This motivates me to try some more combinations.**\n",
        "\n",
        "Naives Bayes:\n",
        "Validation Accuracy: 0.6866866866866866\n",
        "Test Accuracy: 0.6053872053872054\n",
        "\n",
        "Logistic Regression:\n",
        "Validation Accuracy: 0.6836836836836837\n",
        "Test Accuracy: 0.5973063973063973\n",
        "\n",
        "**The fourth test involves handicraft features - number of hate words used. When comparing the results, while insignifantly, the accuracy decreases. This increase is consistent across the two models.** \n",
        "\n",
        "Naives Bayes:\n",
        "Validation Accuracy: 0.6826826826826827\n",
        "Test Accuracy: 0.6026936026936027\n",
        "\n",
        "Logistic Regression:\n",
        "Validation Accuracy: 0.6856856856856857\n",
        "Test Accuracy: 0.5973063973063973\n",
        "\n",
        "\n",
        "**I wished to implement more handcraft models but I realised that because of my pre-processing it becomes difficult for me to check for some cues. The '#' and '@' symbols are removed. Moreover, '!' and '?' have also been pre-processed. It can also be noted that since cleaned data is converted to lowercase, capitalisation cannot be used either. This would be a learning i take from the pre-processing step that I wish to consider my future models.**\n",
        "\n",
        "**These set of experiements have demonstrated that the handicraft features have very little impact on accuracy. The limitations for the same have been discussed.** \n",
        "\n",
        "**Another thing to consider is that the Naives Bayes model tends to showcase slightly better test accuracy than logistic regression. This might be because logistic regression is more prone to overfitting. Moreover, if the feature distribution in the dataset aligns well with the underlying assumptions of Naive Bayes (e.g., features are conditionally independent), it may lead to better performance compared to logistic regression.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob6uxaVfjIP9"
      },
      "source": [
        "<a name='10'></a>\n",
        "## 10. Reflection, Bias, Fairness, Ethics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kpu4SjMkwBxj"
      },
      "source": [
        "<a name='e17'></a>\n",
        "###  Exercise 17 (points 5)\n",
        "\n",
        "There are many different applications for such a hate-speech classification model. For example, social media platforms could use it in order to moderate relevant texts: this actually falls under the obligations of platforms to report on the moderating actions they have taken under the Data Services Act (DSA). You can check this database [here](https://transparency.dsa.ec.europa.eu/) and read more about the type of content being reported [here](https://arxiv.org/pdf/2404.02894.pdf).\n",
        "\n",
        "Systems used in the context of (lega)l decision-making or, more generally, systems that filter specific content (e.g. removing hate-speech comments) should be used with great care and in view of the potential interference\n",
        "with human rights, namely the right to free speech.\n",
        "\n",
        "Based on your experience from this lab (i.e. you saw how easy/difficult is to build such a task, you also saw how easy/difficult is to get good results and you also saw in which cases the model might not perform well enough) you are asked to reflect on the ethical use of such a classification model. More specifically, you are asked to reflect on the following questions (and feel free to expand your analysis).\n",
        "\n",
        "* Are there considerations for the (final) model you built? Did you make decisions (e.g. for the way data is processed) tht might affect the model?\n",
        "* What are the limitations of the model and your results?\n",
        "* Who can be potential relevant stakeholders of such a system? If you had to deploy such a model in practice, who would you need to consult before putting it into practice?\n",
        "* Who can benefit from such a model (and the analysis)? Who can be harmed? Who is excluded?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DQDz3Z15O6i"
      },
      "source": [
        "1. There are definite coniderations that were taken for when and how I built the final model. The first and foremost is attempting to eliminate personal bias. The idea of immigration and misogyny evoke strong sentiments in me. This makes it important for me to not just classify any tweet/information that goes against my own beliefs as hate-speech. That would be a very elitist approach and I hope to have avoided doing so in the process. This is why it would be vastly beneficial for models to be developed in teams. Teams which are diverse and representtive of people from all backgrounds. This is something that we are failing to see with the rapid developments of AI. The most recent example being Gemini and its anti-white repersentation stance. This was problematic and such issues will continue to arrise if the developers go unchecked. \n",
        "As far as my own decisions are concerned, my idea of pre-processing may have caused issues. There were various aspects that my models had to overlook because of said pre-processing. This hindered the ability to make better judgement. I would hypothise that if my cleaning was not as 'intensive' my handcrafted features would have yielded better results.\n",
        "\n",
        "2. As mentioned earlier, the model is limited due to excess or incorrect pre-processing. This means that the data has been too normalised and important stylistic ffeatures have been overlooked or eliminated. This impacts the results. Experiments from Q16 showcase how the model was very limited. \n",
        "Another limitation of the model would be the lack of context consideration. A lot of words tend to be taken at face value. This might not work very well with a lot of tweets. This could be because of the use of slang where meanings of words get altered or just the use of negations or even punctuations. These were features that should not be overlooked. Sarcasm is also not detected with the lack of context. Sarcasm is also a difficult feautre to understand, where even people tend to not understand it at times (my roommate), so it is unfair to assume that machines can. \n",
        "We should also consider that all the language processing is done in English. There is a possibility that people may use words from other languages. This could be done with/without the Latin script that we are all accustomed to.\n",
        "\n",
        "3. The potential stakeholders for such a system is difficult to determine. Firstly and most importantly, the model should be developed by speakers of the language that is being taken into consideration. The model should be consulted by linguists most importantly who understand the evolution of language and trends. In the absence of linguists, historians or journalists might be good alternatives. They are also quick to capture trends who can help us better. It is important to ask why a model is being built and who for. So, it is essential to understand from our clients what the real purpose of this is supposed to be, and then verify and validate if what was expected has been built.\n",
        "It would be smart to have a development team that is representative of the groups involved with such sentiments, if not the whole society as a whole. This would help us get a more nuanced and balanced understanding of how the sentiments can change and how tides shift.\n",
        "Lastly, if possible, analyising past data from where the data is being fetched -analaysing past tweets would be ideal for a twitter sentiment analysis.\n",
        "\n",
        "4. The government and marketting firms can benefit from such models. It helps them understand how the masses are behaving and what they wish to seek. These models could be further used to enhance recommender systems and advertisments accordingly. The government/political parties could also use this information to shift tides and influence how people feel. This could go so far to impact the results of elections. A very notable case being that of Cambridge Analytica and their influence on the elections. Such models could be used to directly attack democracy of a nation. They could be used to spark riots (Arab Spring). \n",
        "While there is a lot of harm that can be done, it can be used for good motives as well. For exmaple, a politican doing sentiment analyiss to make a policy change which reflects what the people want. The options are limitless. \n",
        "\n",
        "The people who are excluded from such are naturally the people who tend to not contribute to such datasets. This could be older people who are more resistant to accept new tech but this could also be people who are excluded from internet access. Where internet is cut off, the voice of people can be shunted. This could vastly impact how we view this world. This makes it incredibly imporatnt to evenly distribute how the data is collected and whom it represents. For example, western world has better access to the internet, which would represent them more than others. But this does not make them more important than others. Such models should not be used to polarise people on critical topics. \n",
        "\n",
        "\n",
        "Special thanks to Srikar Narayan Rao and Garrick Latimer who helped me with this project.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
