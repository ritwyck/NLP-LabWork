# NLP LabWork

Welcome to my repository of NLP assignments completed as part of my university coursework. This collection includes various projects focused on classification models, word embeddings, and the use of pre-trained models. Below you will find an overview of each assignment along with instructions on how to run the code and understand the results.

## Table of Contents
1. [Assignment 1: Classification Models](#assignment-1-classification-models)
2. [Assignment 2: Word Embeddings](#assignment-2-word-embeddings)
3. [Assignment 3: Using Pre-trained Models](#assignment-3-using-pre-trained-models)
4. [How to Run the Code](#how-to-run-the-code)
5. [Results](#results)
6. [References](#references)

## Assignment 1: Classification Models
In this assignment, we explored various classification models for natural language processing tasks. The primary objectives were:
- Understanding the fundamentals of text classification.
- Implementing different classification algorithms such as Naive Bayes and logistic regression.
- Evaluating the performance of each model using metrics like accuracy, precision, recall, and F1-score.

### Files
- `Hate-Tweet-Classification.ipynb`: Contains the code for training and evaluating classification models.

### Usage
To train and evaluate the classification models, run the following command:
```bash
python Hate-Tweet-Classification.ipynb
```

## Assignment 2: Word Embeddings
This assignment focused on creating and utilizing word embeddings for NLP tasks. Key learning outcomes included:
- Understanding word embeddings and their significance in NLP.
- Training custom word embeddings using Word2Vec.

### Files
- `Word-Embeddings.ipynb`: Script for training word embeddings and performing related tasks.

### Usage
To train custom word embeddings or use pre-trained ones, execute:
```bash
python Word-Embeddings.ipynb
```

## Assignment 3: Using Pre-trained Models
In this assignment, we leveraged pre-trained NLP models to solve complex tasks efficiently. The objectives were:
- Understanding the benefits of using pre-trained models.
- Applying models like BERT and other transformers for text classification, sentiment analysis, and more.
- Fine-tuning pre-trained models for specific tasks.

### Files
- `Pre-Trained-Models.ipynb`: Code for applying and fine-tuning pre-trained models.

### Usage
To apply or fine-tune pre-trained models, run:
```bash
python Pre-Trained-Models.ipynb
```

## How to Run the Code
1. Clone this repository:
    ```bash
    git clone https://github.com/ritwyck/NLP-LabWork.git
    cd NLP-LabWork
    ```
2. Install the required dependencies:
    ```bash
    pip install -r requirements.txt
    ```
3. Follow the usage instructions for each assignment.

## Results
Detailed analysis and discussions can be found in the corresponding Jupyter notebooks for each assignment.

## References
- [Word2Vec](https://arxiv.org/abs/1301.3781)
- [BERT](https://arxiv.org/abs/1810.04805)
- [GPT-3](https://arxiv.org/abs/2005.14165)

Feel free to explore the code, use it as a reference for your own projects, and provide feedback or suggestions for improvement.
